{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_1:0\", shape=(), dtype=float32) node2: Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 1. 그래프 빌드\n",
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # tf.float32가 디폴트\n",
    "node3 = tf.add(node1, node2)\n",
    "\n",
    "print(\"node1:\", node1, \"node2:\", node2)\n",
    "print(\"node3:\", node3)  # 결과값이 전혀 나오지 않음. 그냥 이런 텐서이다 라고 얘기하는 정도."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 그래프 실행\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "# 3. 결과값 리턴\n",
    "print(sess.run([node1, node2]))\n",
    "print(sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우 과정\n",
    "\n",
    "1. 그래프 빌드\n",
    "2. 그래프 실행\n",
    "3. 그래프 업데이트 혹은 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "# 앞에서는 a, b를 미리 정해두고 실행했는데 이제는 실행할때 값을 주고싶다. - placeholder\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b\n",
    "\n",
    "# 실행할 노드, 노드들에 필요한 값을 주는 feed_dict - 딕셔너리 형태로 받는다.\n",
    "# sess.run(operation,feed_dict = {x: x_data})\n",
    "print(sess.run(adder_node, feed_dict = {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict = {a: [1, 3], b: [2, 4]})) \n",
    "# 여러개 집어넣을 때는 리스트로 묶어서 넘겨준다. 노드 하나당은 하나만 받는다고 생각. 리스트 하나를 받는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor에서는 rank, shape, type 세개가 중요하다\n",
    "\n",
    "```python\n",
    "t = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "```\n",
    "\n",
    "tensor t는\n",
    "\n",
    "- rank(차원) 2를 가지고\\\n",
    "- shape(차원의 모양)은 (3, 3)이다.\n",
    "    - (바깥 리스트가 가지고 있는 리스트 3개, 각 안쪽 리스트들이 가지고 있는 요소 3개)\n",
    "- type은 대부분 tf.float32 혹은 tf.int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "# 여기서 tf.Variable은 텐서플로우가 자체적으로 변경시키는 값. trainable한 variable\n",
    "# 만들때는 꼭 rank와 shape을 줘야함 rank1, shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = x_train * W + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [1., 2., 3., 4.]\n",
    "mean = tf.reduce_mean(t)\n",
    "sess.run(mean)  # 그냥 평균내주는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# W, b라는 tf.Variable을 만들었음. 이걸 이용해 실행하기 전에는 꼭 tf.global_variables_initializer()해줘야함.\n",
    "# 초기화를 해주는건데 이렇게 되어야 Variable이 완전히 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\t13.282539368\t-0.321580887\t-0.837952435\n",
      "  20\t0.126390636\t0.955282331\t-0.264199406\n",
      "  40\t0.006560382\t1.073052049\t-0.200900495\n",
      "  60\t0.004976959\t1.080632567\t-0.186613947\n",
      "  80\t0.004511273\t1.077891946\t-0.177382469\n",
      " 100\t0.004097127\t1.074331284\t-0.169002250\n",
      " 120\t0.003721080\t1.070847392\t-0.161055654\n",
      " 140\t0.003379544\t1.067518711\t-0.153486282\n",
      " 160\t0.003069360\t1.064345717\t-0.146273002\n",
      " 180\t0.002787639\t1.061321616\t-0.139398694\n",
      " 200\t0.002531781\t1.058439732\t-0.132847399\n",
      " 220\t0.002299397\t1.055693269\t-0.126603991\n",
      " 240\t0.002088351\t1.053075910\t-0.120654054\n",
      " 260\t0.001896677\t1.050581574\t-0.114983812\n",
      " 280\t0.001722588\t1.048204422\t-0.109579958\n",
      " 300\t0.001564482\t1.045938969\t-0.104430132\n",
      " 320\t0.001420889\t1.043780088\t-0.099522293\n",
      " 340\t0.001290477\t1.041722655\t-0.094845146\n",
      " 360\t0.001172035\t1.039761662\t-0.090387881\n",
      " 380\t0.001064458\t1.037892938\t-0.086139895\n",
      " 400\t0.000966753\t1.036112189\t-0.082091577\n",
      " 420\t0.000878024\t1.034415126\t-0.078233577\n",
      " 440\t0.000797435\t1.032797694\t-0.074556924\n",
      " 460\t0.000724244\t1.031256437\t-0.071053036\n",
      " 480\t0.000657767\t1.029787421\t-0.067713872\n",
      " 500\t0.000597398\t1.028387547\t-0.064531572\n",
      " 520\t0.000542569\t1.027053475\t-0.061498854\n",
      " 540\t0.000492770\t1.025782108\t-0.058608636\n",
      " 560\t0.000447541\t1.024570465\t-0.055854298\n",
      " 580\t0.000406465\t1.023415685\t-0.053229339\n",
      " 600\t0.000369155\t1.022315264\t-0.050727740\n",
      " 620\t0.000335274\t1.021266699\t-0.048343800\n",
      " 640\t0.000304499\t1.020266891\t-0.046071779\n",
      " 660\t0.000276550\t1.019314408\t-0.043906357\n",
      " 680\t0.000251165\t1.018406630\t-0.041842837\n",
      " 700\t0.000228114\t1.017541647\t-0.039876334\n",
      " 720\t0.000207176\t1.016717196\t-0.038002290\n",
      " 740\t0.000188158\t1.015931487\t-0.036216285\n",
      " 760\t0.000170889\t1.015182853\t-0.034514233\n",
      " 780\t0.000155204\t1.014469385\t-0.032892186\n",
      " 800\t0.000140961\t1.013789415\t-0.031346411\n",
      " 820\t0.000128023\t1.013141274\t-0.029873291\n",
      " 840\t0.000116273\t1.012523770\t-0.028469373\n",
      " 860\t0.000105601\t1.011935234\t-0.027131431\n",
      " 880\t0.000095908\t1.011374235\t-0.025856344\n",
      " 900\t0.000087105\t1.010839701\t-0.024641199\n",
      " 920\t0.000079110\t1.010330439\t-0.023483185\n",
      " 940\t0.000071850\t1.009844899\t-0.022379659\n",
      " 960\t0.000065254\t1.009382129\t-0.021327872\n",
      " 980\t0.000059265\t1.008941174\t-0.020325545\n",
      "1000\t0.000053826\t1.008521080\t-0.019370312\n",
      "1020\t0.000048885\t1.008120537\t-0.018459998\n",
      "1040\t0.000044398\t1.007738948\t-0.017592415\n",
      "1060\t0.000040324\t1.007375240\t-0.016765630\n",
      "1080\t0.000036623\t1.007028699\t-0.015977703\n",
      "1100\t0.000033262\t1.006698489\t-0.015226893\n",
      "1120\t0.000030209\t1.006383538\t-0.014511301\n",
      "1140\t0.000027437\t1.006083608\t-0.013829350\n",
      "1160\t0.000024918\t1.005797625\t-0.013179432\n",
      "1180\t0.000022630\t1.005525112\t-0.012560039\n",
      "1200\t0.000020553\t1.005265594\t-0.011969755\n",
      "1220\t0.000018667\t1.005018115\t-0.011407305\n",
      "1240\t0.000016954\t1.004782200\t-0.010871208\n",
      "1260\t0.000015398\t1.004557490\t-0.010360273\n",
      "1280\t0.000013984\t1.004343390\t-0.009873391\n",
      "1300\t0.000012701\t1.004139185\t-0.009409389\n",
      "1320\t0.000011535\t1.003944755\t-0.008967218\n",
      "1340\t0.000010477\t1.003759384\t-0.008545828\n",
      "1360\t0.000009516\t1.003582716\t-0.008144238\n",
      "1380\t0.000008642\t1.003414392\t-0.007761516\n",
      "1400\t0.000007849\t1.003253818\t-0.007396751\n",
      "1420\t0.000007129\t1.003100991\t-0.007049164\n",
      "1440\t0.000006474\t1.002955198\t-0.006717886\n",
      "1460\t0.000005880\t1.002816439\t-0.006402196\n",
      "1480\t0.000005340\t1.002683997\t-0.006101333\n",
      "1500\t0.000004850\t1.002557874\t-0.005814618\n",
      "1520\t0.000004405\t1.002437711\t-0.005541393\n",
      "1540\t0.000004001\t1.002323151\t-0.005280981\n",
      "1560\t0.000003634\t1.002213955\t-0.005032826\n",
      "1580\t0.000003300\t1.002109885\t-0.004796308\n",
      "1600\t0.000002997\t1.002010822\t-0.004570948\n",
      "1620\t0.000002722\t1.001916409\t-0.004356207\n",
      "1640\t0.000002473\t1.001826406\t-0.004151558\n",
      "1660\t0.000002246\t1.001740575\t-0.003956517\n",
      "1680\t0.000002040\t1.001658678\t-0.003770646\n",
      "1700\t0.000001853\t1.001580954\t-0.003593511\n",
      "1720\t0.000001683\t1.001506567\t-0.003424719\n",
      "1740\t0.000001528\t1.001435995\t-0.003263860\n",
      "1760\t0.000001388\t1.001368403\t-0.003110571\n",
      "1780\t0.000001261\t1.001304150\t-0.002964437\n",
      "1800\t0.000001145\t1.001242995\t-0.002825203\n",
      "1820\t0.000001040\t1.001184583\t-0.002692513\n",
      "1840\t0.000000945\t1.001128912\t-0.002566044\n",
      "1860\t0.000000858\t1.001075864\t-0.002445521\n",
      "1880\t0.000000779\t1.001025438\t-0.002330724\n",
      "1900\t0.000000708\t1.000977278\t-0.002221302\n",
      "1920\t0.000000643\t1.000931501\t-0.002117042\n",
      "1940\t0.000000584\t1.000887871\t-0.002017688\n",
      "1960\t0.000000531\t1.000846148\t-0.001922965\n",
      "1980\t0.000000482\t1.000806332\t-0.001832679\n",
      "2000\t0.000000438\t1.000768304\t-0.001746653\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print('{:4d}\\t{:.9f}\\t{:.9f}\\t{:.9f}'.format(step, sess.run(cost), *sess.run(W), *sess.run(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder 이용. x, y가 미리 정해져있지 않고 데이터를 바꿔가며 써보고싶을때 쓰면 된다.\n",
    "X = tf.placeholder(tf.float32, shape = None)\n",
    "y = tf.placeholder(tf.float32, shape = None)\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'Weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = W * X + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\t6.082481861\t0.156250954\t-0.394155055\n",
      "  20\t0.055196181\t0.926926851\t-0.054053050\n",
      "  40\t0.000544122\t0.999968469\t-0.020892339\n",
      "  60\t0.000044596\t1.006597996\t-0.016994869\n",
      "  80\t0.000036427\t1.006918907\t-0.015918501\n",
      " 100\t0.000033046\t1.006653786\t-0.015143873\n",
      " 120\t0.000030013\t1.006346941\t-0.014429711\n",
      " 140\t0.000027259\t1.006049156\t-0.013751370\n",
      " 160\t0.000024756\t1.005764842\t-0.013105074\n",
      " 180\t0.000022484\t1.005493999\t-0.012489151\n",
      " 200\t0.000020420\t1.005235791\t-0.011902190\n",
      " 220\t0.000018546\t1.004989624\t-0.011342786\n",
      " 240\t0.000016844\t1.004755259\t-0.010809689\n",
      " 260\t0.000015297\t1.004531741\t-0.010301689\n",
      " 280\t0.000013894\t1.004318833\t-0.009817555\n",
      " 300\t0.000012619\t1.004115939\t-0.009356239\n",
      " 320\t0.000011460\t1.003922462\t-0.008916588\n",
      " 340\t0.000010409\t1.003738046\t-0.008497549\n",
      " 360\t0.000009454\t1.003562450\t-0.008098217\n",
      " 380\t0.000008586\t1.003395081\t-0.007717690\n",
      " 400\t0.000007798\t1.003235459\t-0.007355000\n",
      " 420\t0.000007082\t1.003083467\t-0.007009346\n",
      " 440\t0.000006432\t1.002938509\t-0.006679959\n",
      " 460\t0.000005842\t1.002800465\t-0.006366075\n",
      " 480\t0.000005306\t1.002668977\t-0.006066955\n",
      " 500\t0.000004819\t1.002543449\t-0.005781864\n",
      " 520\t0.000004377\t1.002424002\t-0.005510151\n",
      " 540\t0.000003975\t1.002310038\t-0.005251217\n",
      " 560\t0.000003610\t1.002201438\t-0.005004485\n",
      " 580\t0.000003279\t1.002098083\t-0.004769299\n",
      " 600\t0.000002978\t1.001999497\t-0.004545240\n",
      " 620\t0.000002705\t1.001905560\t-0.004331687\n",
      " 640\t0.000002456\t1.001816034\t-0.004128195\n",
      " 660\t0.000002231\t1.001730800\t-0.003934220\n",
      " 680\t0.000002026\t1.001649499\t-0.003749388\n",
      " 700\t0.000001840\t1.001572013\t-0.003573281\n",
      " 720\t0.000001672\t1.001498222\t-0.003405424\n",
      " 740\t0.000001518\t1.001427889\t-0.003245509\n",
      " 760\t0.000001379\t1.001360655\t-0.003093062\n",
      " 780\t0.000001253\t1.001296878\t-0.002947781\n",
      " 800\t0.000001138\t1.001235962\t-0.002809326\n",
      " 820\t0.000001033\t1.001177907\t-0.002677381\n",
      " 840\t0.000000938\t1.001122475\t-0.002551612\n",
      " 860\t0.000000853\t1.001069784\t-0.002431800\n",
      " 880\t0.000000774\t1.001019478\t-0.002317593\n",
      " 900\t0.000000703\t1.000971675\t-0.002208769\n",
      " 920\t0.000000639\t1.000926137\t-0.002105081\n",
      " 940\t0.000000580\t1.000882745\t-0.002006258\n",
      " 960\t0.000000527\t1.000841379\t-0.001912068\n",
      " 980\t0.000000479\t1.000801921\t-0.001822307\n",
      "1000\t0.000000435\t1.000764012\t-0.001736801\n",
      "1020\t0.000000395\t1.000728250\t-0.001655313\n",
      "1040\t0.000000359\t1.000694275\t-0.001577604\n",
      "1060\t0.000000326\t1.000661492\t-0.001503542\n",
      "1080\t0.000000296\t1.000630498\t-0.001433050\n",
      "1100\t0.000000269\t1.000601172\t-0.001365777\n",
      "1120\t0.000000244\t1.000572562\t-0.001301771\n",
      "1140\t0.000000222\t1.000546098\t-0.001240717\n",
      "1160\t0.000000202\t1.000520229\t-0.001182498\n",
      "1180\t0.000000183\t1.000496149\t-0.001127083\n",
      "1200\t0.000000166\t1.000472546\t-0.001074231\n",
      "1220\t0.000000151\t1.000450730\t-0.001023867\n",
      "1240\t0.000000137\t1.000429273\t-0.000975924\n",
      "1260\t0.000000125\t1.000409365\t-0.000930092\n",
      "1280\t0.000000113\t1.000390291\t-0.000886625\n",
      "1300\t0.000000103\t1.000371695\t-0.000844982\n",
      "1320\t0.000000094\t1.000354648\t-0.000805397\n",
      "1340\t0.000000085\t1.000337958\t-0.000767812\n",
      "1360\t0.000000077\t1.000321865\t-0.000731705\n",
      "1380\t0.000000070\t1.000307083\t-0.000697443\n",
      "1400\t0.000000064\t1.000292778\t-0.000664927\n",
      "1420\t0.000000058\t1.000278592\t-0.000633713\n",
      "1440\t0.000000053\t1.000265837\t-0.000603927\n",
      "1460\t0.000000048\t1.000253677\t-0.000575761\n",
      "1480\t0.000000043\t1.000241756\t-0.000548994\n",
      "1500\t0.000000039\t1.000229955\t-0.000523203\n",
      "1520\t0.000000036\t1.000219345\t-0.000498569\n",
      "1540\t0.000000033\t1.000209332\t-0.000475288\n",
      "1560\t0.000000030\t1.000199795\t-0.000453240\n",
      "1580\t0.000000027\t1.000190258\t-0.000432185\n",
      "1600\t0.000000024\t1.000180960\t-0.000411807\n",
      "1620\t0.000000022\t1.000172615\t-0.000392384\n",
      "1640\t0.000000020\t1.000164747\t-0.000374020\n",
      "1660\t0.000000018\t1.000157237\t-0.000356662\n",
      "1680\t0.000000017\t1.000150084\t-0.000340222\n",
      "1700\t0.000000015\t1.000142932\t-0.000324490\n",
      "1720\t0.000000014\t1.000135779\t-0.000309233\n",
      "1740\t0.000000013\t1.000129461\t-0.000294527\n",
      "1760\t0.000000011\t1.000123501\t-0.000280638\n",
      "1780\t0.000000010\t1.000117779\t-0.000267500\n",
      "1800\t0.000000009\t1.000112414\t-0.000255080\n",
      "1820\t0.000000009\t1.000107408\t-0.000243330\n",
      "1840\t0.000000008\t1.000102639\t-0.000232242\n",
      "1860\t0.000000007\t1.000097871\t-0.000221669\n",
      "1880\t0.000000006\t1.000093102\t-0.000211441\n",
      "1900\t0.000000006\t1.000088334\t-0.000201441\n",
      "1920\t0.000000005\t1.000084162\t-0.000191753\n",
      "1940\t0.000000005\t1.000080228\t-0.000182594\n",
      "1960\t0.000000004\t1.000076532\t-0.000173944\n",
      "1980\t0.000000004\t1.000072956\t-0.000165769\n",
      "2000\t0.000000004\t1.000069618\t-0.000158038\n",
      "5.0001903\n",
      "2.500016\n",
      "1.4999464 3.5000856\n",
      "[1.0000696] [-0.00015804]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict = {X: [1, 2, 3], y: [1, 2, 3]})\n",
    "        \n",
    "    if step % 20 == 0:\n",
    "        print('{:4d}\\t{:.9f}\\t{:.9f}\\t{:.9f}'.format(step, cost_val, *W_val, *b_val))\n",
    "\n",
    "print(*sess.run(hypothesis, feed_dict = {X: [5]}))\n",
    "print(*sess.run(hypothesis, feed_dict = {X: [2.5]}))\n",
    "print(*sess.run(hypothesis, feed_dict = {X: [1.5, 3.5]}))\n",
    "print(*sess.run([W, b]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\t0.634475768\t1.272931576\t-1.246247053\n",
      "  20\t0.175610498\t1.457960486\t-1.100641012\n",
      "  40\t0.155859873\t1.455277801\t-1.040627480\n",
      "  60\t0.141521543\t1.435675383\t-0.990932763\n",
      "  80\t0.128531888\t1.415371060\t-0.944287479\n",
      " 100\t0.116734706\t1.395866513\t-0.899902344\n",
      " 120\t0.106020302\t1.377263665\t-0.857609570\n",
      " 140\t0.096289285\t1.359533668\t-0.817304909\n",
      " 160\t0.087451495\t1.342636943\t-0.778894484\n",
      " 180\t0.079424843\t1.326534271\t-0.742289305\n",
      " 200\t0.072134860\t1.311188340\t-0.707404375\n",
      " 220\t0.065514058\t1.296563745\t-0.674158990\n",
      " 240\t0.059500936\t1.282626390\t-0.642476141\n",
      " 260\t0.054039732\t1.269343853\t-0.612282097\n",
      " 280\t0.049079750\t1.256685734\t-0.583507121\n",
      " 300\t0.044575002\t1.244622231\t-0.556084335\n",
      " 320\t0.040483687\t1.233126044\t-0.529950321\n",
      " 340\t0.036767948\t1.222169995\t-0.505044580\n",
      " 360\t0.033393245\t1.211728811\t-0.481309444\n",
      " 380\t0.030328289\t1.201778412\t-0.458689749\n",
      " 400\t0.027544672\t1.192295671\t-0.437133133\n",
      " 420\t0.025016522\t1.183258533\t-0.416589558\n",
      " 440\t0.022720382\t1.174646020\t-0.397011399\n",
      " 460\t0.020635031\t1.166438341\t-0.378353387\n",
      " 480\t0.018741067\t1.158616304\t-0.360572189\n",
      " 500\t0.017020933\t1.151161790\t-0.343626499\n",
      " 520\t0.015458684\t1.144057870\t-0.327477247\n",
      " 540\t0.014039810\t1.137287617\t-0.312087059\n",
      " 560\t0.012751192\t1.130835533\t-0.297420084\n",
      " 580\t0.011580828\t1.124686837\t-0.283442348\n",
      " 600\t0.010517896\t1.118826866\t-0.270121634\n",
      " 620\t0.009552523\t1.113242507\t-0.257426858\n",
      " 640\t0.008675738\t1.107920527\t-0.245328695\n",
      " 660\t0.007879461\t1.102848768\t-0.233799219\n",
      " 680\t0.007156249\t1.098015189\t-0.222811535\n",
      " 700\t0.006499422\t1.093408823\t-0.212340176\n",
      " 720\t0.005902875\t1.089018941\t-0.202360913\n",
      " 740\t0.005361081\t1.084835410\t-0.192850724\n",
      " 760\t0.004869032\t1.080848694\t-0.183787540\n",
      " 780\t0.004422131\t1.077048898\t-0.175150216\n",
      " 800\t0.004016250\t1.073427916\t-0.166918814\n",
      " 820\t0.003647624\t1.069976926\t-0.159074202\n",
      " 840\t0.003312828\t1.066688299\t-0.151598260\n",
      " 860\t0.003008758\t1.063554168\t-0.144473687\n",
      " 880\t0.002732605\t1.060567737\t-0.137684107\n",
      " 900\t0.002481804\t1.057721019\t-0.131213471\n",
      " 920\t0.002254002\t1.055008292\t-0.125046894\n",
      " 940\t0.002047131\t1.052423120\t-0.119170137\n",
      " 960\t0.001859232\t1.049959421\t-0.113569550\n",
      " 980\t0.001688587\t1.047611594\t-0.108232208\n",
      "1000\t0.001533601\t1.045374036\t-0.103145696\n",
      "1020\t0.001392840\t1.043241620\t-0.098298252\n",
      "1040\t0.001265001\t1.041209579\t-0.093678653\n",
      "1060\t0.001148900\t1.039272666\t-0.089276209\n",
      "1080\t0.001043445\t1.037427068\t-0.085080549\n",
      "1100\t0.000947673\t1.035668135\t-0.081082039\n",
      "1120\t0.000860694\t1.033991933\t-0.077271499\n",
      "1140\t0.000781694\t1.032394290\t-0.073639989\n",
      "1160\t0.000709947\t1.030871868\t-0.070179135\n",
      "1180\t0.000644784\t1.029420972\t-0.066880926\n",
      "1200\t0.000585603\t1.028038383\t-0.063737765\n",
      "1220\t0.000531855\t1.026720762\t-0.060742360\n",
      "1240\t0.000483039\t1.025464892\t-0.057887726\n",
      "1260\t0.000438703\t1.024268150\t-0.055167239\n",
      "1280\t0.000398439\t1.023127675\t-0.052574590\n",
      "1300\t0.000361872\t1.022040725\t-0.050103862\n",
      "1320\t0.000328657\t1.021004796\t-0.047749218\n",
      "1340\t0.000298492\t1.020017624\t-0.045505136\n",
      "1360\t0.000271091\t1.019076824\t-0.043366387\n",
      "1380\t0.000246207\t1.018180370\t-0.041328210\n",
      "1400\t0.000223609\t1.017325878\t-0.039385892\n",
      "1420\t0.000203085\t1.016511559\t-0.037534852\n",
      "1440\t0.000184446\t1.015735626\t-0.035770819\n",
      "1460\t0.000167515\t1.014996052\t-0.034089688\n",
      "1480\t0.000152142\t1.014291286\t-0.032487594\n",
      "1500\t0.000138177\t1.013619781\t-0.030960828\n",
      "1520\t0.000125494\t1.012979627\t-0.029505795\n",
      "1540\t0.000113976\t1.012369633\t-0.028119121\n",
      "1560\t0.000103515\t1.011788368\t-0.026797641\n",
      "1580\t0.000094015\t1.011234403\t-0.025538286\n",
      "1600\t0.000085386\t1.010706425\t-0.024338115\n",
      "1620\t0.000077549\t1.010203362\t-0.023194393\n",
      "1640\t0.000070432\t1.009723663\t-0.022104414\n",
      "1660\t0.000063966\t1.009266734\t-0.021065522\n",
      "1680\t0.000058095\t1.008831263\t-0.020075481\n",
      "1700\t0.000052763\t1.008416176\t-0.019132014\n",
      "1720\t0.000047921\t1.008020639\t-0.018232873\n",
      "1740\t0.000043521\t1.007643700\t-0.017375989\n",
      "1760\t0.000039528\t1.007284522\t-0.016559387\n",
      "1780\t0.000035899\t1.006942272\t-0.015781188\n",
      "1800\t0.000032605\t1.006615996\t-0.015039588\n",
      "1820\t0.000029612\t1.006305099\t-0.014332814\n",
      "1840\t0.000026894\t1.006008744\t-0.013659238\n",
      "1860\t0.000024426\t1.005726337\t-0.013017318\n",
      "1880\t0.000022185\t1.005457282\t-0.012405590\n",
      "1900\t0.000020149\t1.005200863\t-0.011822598\n",
      "1920\t0.000018299\t1.004956484\t-0.011267023\n",
      "1940\t0.000016619\t1.004723430\t-0.010737493\n",
      "1960\t0.000015094\t1.004501462\t-0.010232856\n",
      "1980\t0.000013709\t1.004289865\t-0.009751945\n",
      "2000\t0.000012451\t1.004088283\t-0.009293650\n"
     ]
    }
   ],
   "source": [
    "# Lab 2 Linear Regression\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = x_data * W + b\n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let TensorFlow figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Fit the line\n",
    "    for step in range(2001):\n",
    "        _, cost_val, W_val, b_val = sess.run([train, cost, W, b])\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print('{:4d}\\t{:.9f}\\t{:.9f}\\t{:.9f}'.format(step, cost_val, *W_val, *b_val))\n",
    "\n",
    "# Learns best fit W:[ 1.],  b:[ 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)  # W를 placeholder로 줬다는거는 임의로 이 값을 바꿔볼 것이다.\n",
    "hypothesis = X * W  # 편의상 b는 생략한 모델을 생각해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val = []\n",
    "cost_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1  # feed_W를 -3부터 5까지 옮겨가며 cost와 W를 계산할 것\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict = {W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3yV5f3/8dcnO5BFIAmZhD1kBIgBREEZVgVZakURcbRoa61Vq9WfHbbWOqvVrxNnXODCuhBEBEFBIGwwQMggCSM7kAGZ1++PHCy1AU4g59xnfJ6PRx5n5CT3WyRv7lznuq9LjDEopZRyPz5WB1BKKXV6tMCVUspNaYErpZSb0gJXSik3pQWulFJuys+ZB+vSpYtJTk525iGVUsrtbdiwodQYE/XT551a4MnJyWRkZDjzkEop5fZEZG9rz+sQilJKuSktcKWUclNa4Eop5aa0wJVSyk1pgSullJvSAldKKTelBa6UUm7KLQr8860HeHttq9MglVLKa7lFgS/adoDHl+yirrHJ6ihKKeUy3KLAZ6YlUlHbwJIdRVZHUUopl+EWBT66ZxcSI4NZsC7f6ihKKeUy3KLAfXyEK1MTWZ1dRl5pjdVxlFLKJbhFgQNckZqIr4+wYH2B1VGUUsoluE2Bx4QFcUHfaD7YUEhDU7PVcZRSynJuU+AAV6UlUlpdx7JMfTNTKaXcqsDH9omia1gQ89fpMIpSSrlVgfv5+vDz1ARWZpVQWFFrdRyllLLUKQtcRPqKyObjPg6LyO9EJFJElopIlu22kzMC//zsRADeyyh0xuGUUuqMbC2s5LLnV7OnuLrdv/cpC9wYs8sYk2KMSQGGA7XAR8A9wDJjTG9gme2xwyV06sCY3lG8uz6fRn0zUynl4t5Zm88P+w8THRbY7t+7rUMo44FsY8xeYCqQbns+HZjWnsFOZtaIJIoO1/H1zmJnHVIppdrs8NEGPt68nylD4ggL8m/379/WAp8JzLfdjzHGHLDdPwjEtFuqUxjXL5quYUG8vVavzFRKua5/b9rHkYYmZo1Mcsj3t7vARSQAmAK8/9PPGWMMYE7wdXNFJENEMkpKSk476PH8fH248uxEVmaVUFCub2YqpVyPMYZ31uYzKD6cwQkRDjlGW87ALwY2GmOOTcIuEpFYANttq+MZxph5xphUY0xqVFTUmaU9zsy0RASYr+ujKKVc0Mb8CnYerGLWCMecfUPbCvwq/jN8AvAJMMd2fw7wcXuFskdseDDj+sXwXkYB9Y36ZqZSyrW8/X0+IYF+XDokzmHHsKvARaQjMBFYeNzTDwMTRSQLmGB77FSzRiZRWl3Plz8cdPahlVLqhCpq6vls2wGmD42nY6Cfw45j13c2xtQAnX/yXBkts1IsM6Z3FAmdgnlnbT6TBzvuXzmllGqLDzcWUt/YzNUOHD4BN7sS86d8fYSr0pJYnV1Gdkn7T5JXSqm2Ovbm5bCkCPrHhjn0WG5d4ABXpCbg5yO8o1MKlVIuYE1OGTmlNVw9opvDj+X2BR4dGsRFA7vyfkYBR+p1z0yllLXeXLOXiA7+TB4c6/BjuX2BA8we2Y3DRxv5dMt+q6MopbzYwUNH+fKHIq5MTSTI39fhx/OIAk/rHknfmFDe+D6PlmuKlFLK+eavy6fZGGY5YfgEPKTARYRrRnVj+77DbC6otDqOUsoLNTQ1M39dPuf3iSKpcwenHNMjChxg+tB4QgL9eHPNXqujKKW80Jc7iiiuqmP2KOecfYMHFXhIoB8zhsXz2dYDlNfUWx1HKeVl3vw+j8TIYMb2iXbaMT2mwAGuGdmN+qZm3tWd65VSTrS7qIrvc8qZNaIbvj7itON6VIH3iQllZI9I3l67l6ZmfTNTKeUcb32/lwA/H36emujU43pUgQPMHplMYcURVuzSzR6UUo5XXdfIwo37mDw4lsiOAU49tscV+IVnxdA1LIjXV+dZHUUp5QU+3FBIdV0jc0YlO/3YHlfg/r4+zBqRxKqsUodsIqqUUsc0NxvS1+SRkhjBkETHbNpwMh5X4ABXjUgiwNeHN9bkWR1FKeXBVu0pJaekhutHJ1tyfI8s8C4hgUweEsuHGwqpOtpgdRyllIdKX51HVGggFw90/LonrfHIAge47pxkauqb+GBDodVRlFIeKK+0huW7irk6LYkAP2uq1GMLfHBCBMOSIkhfnUezTilUSrWzN9bsxc9HHLrn5al4bIEDzDknmbyyWr7JKrE6ilLKg9TUNfJ+RgGXDIolOizIshz27okZISIfiMhOEckUkVEiEikiS0Uky3bbydFh2+rigbFEhQaSrlMKlVLtaOHGQqrqGplzTrKlOew9A38KWGyM6QcMATKBe4BlxpjewDLbY5cS4OfDNSO6sWJXCTm65ZpSqh00NxteX53HkIRwhlowdfB4pyxwEQkHxgCvABhj6o0xlcBUIN32snRgmqNCnomrbVMK9cIepVR7WJlVQnZJDdeNTkbEeeuetMaeM/DuQAnwmohsEpGXRaQjEGOMOWB7zUEgprUvFpG5IpIhIhklJc4fi44KDWRKShzvZxRyqFanFCqlzsyr3+URHRrIpEFxVkexq8D9gGHA88aYoUANPxkuMS3b4LQ61cMYM88Yk2qMSY2KijrTvKflhtHdOdLQxIL1uvGxUur0ZRVVsXJ3CdeO6mbZ1MHj2ZOgECg0xqy1Pf6AlkIvEpFYANuty64eNSAujFE9OpO+Oo/Gpmar4yil3NSr3+UR6OfjlB3n7XHKAjfGHAQKRKSv7anxwA/AJ8Ac23NzgI8dkrCd3HBud/YfOsriHQetjqKUckMVNfUs3FjIjGHxTl918ET87HzdrcDbIhIA5ADX01L+74nIjcBe4OeOidg+xveLplvnDrz6bS6TB1s/dqWUci/vrMunrrGZG0Z3tzrKj+wqcGPMZiC1lU+Nb984juPjI1x/TjL3f/oDm/IrGJrkctPWlVIuqr6xmTfW5HFe7y70jgm1Os6PrB+Fd6LLUxMJDfTj1e/yrI6ilHIjX2w/QNHhOm4413XOvsHLCjwk0I+ZaYks2naAfZVHrI6jlHIDxhhe+TaXnlEdGdvbmpl0J+JVBQ5wnW386vXvci1OopRyB2tzy9laeIgbz+2BjxM3LLaH1xV4fEQwkwbFMn9dAYd1rXCl1Cm8tDKHzh0DmDEs3uoo/8PrChzgl+f1oLqukXfXFVgdRSnlwvYUV7FsZzHXjkomyN/X6jj/wysLfFBCOCN7RPLqd7k06IU9SqkTeOXbXAL9fLhmpHVrfp+MVxY4wNwxPThw6Cifbz1w6hcrpbxOSVUdH27cx+XDE+gcEmh1nFZ5bYGf3yeaXtEhvLQqh5alXJRS6j/eXJNHQ1MzN7rY1MHjeW2B+/gIvzi3Ozv2H2ZNdpnVcZRSLuRIfRNvfL+XCf1j6BEVYnWcE/LaAgeYNjSeLiEBzFuVY3UUpZQL+WBDAZW1Dcwd08PqKCfl1QUe5O/LnFHJrNhVws6Dh62Oo5RyAY1Nzby0KpeUxAhSu7n2khteXeAAs0d1o0OALy9+o2fhSin4YvtB8struXlsT8t33DkVry/wiA4BXJWWxCdb9lNQXmt1HKWUhYwxvPBNNj2iOnLhgFY3GXMpXl/gAL84rzs+0jLnUynlvb7dU8qO/Ye5aYzrXTbfGi1wIDY8mKkp8SxYn095Tb3VcZRSFnl+RTYxYYFMG+p6l823Rgvc5uaxPTja0Ey67l6vlFfaWljJ6uwybhjdnUA/17tsvjVa4Da9okOZ0D+G9DV51NY3Wh1HKeVkL3yTTWiQH1ePcM3L5ltjV4GLSJ6IbBORzSKSYXsuUkSWikiW7da159vY4Vfn96SytoEFusiVUl4lt7SGL7YfZPbIboQG+Vsdx25tOQO/wBiTYow5trXaPcAyY0xvYJntsVsb3q0TacmRvLQqh/pGXeRKKW/x4jfZ+Pv6cN3oZKujtMmZDKFMBdJt99OBaWcex3q/vqAnBw4d5d+b9lkdRSnlBPsrj/DhxkKuTE0kOjTI6jhtYm+BG+BLEdkgInNtz8UYY44t5XcQcP1Jk3YY2yeKgfFhPP9NNk3NusiVUp6uZUE7uGmsa1823xp7C/xcY8ww4GLgFhEZc/wnTctyfq22nYjMFZEMEckoKSk5s7ROICLccn4vcktr+HybLjWrlCcrra5j/rp8pqbEk9Cpg9Vx2syuAjfG7LPdFgMfAWlAkYjEAthui0/wtfOMManGmNSoKNfaEPREfnZWV3pFh/Dc8j0061m4Uh7r1W9zqWts5tcX9LQ6ymk5ZYGLSEcRCT12H7gQ2A58AsyxvWwO8LGjQjqbj4/w6/N7svNgFV/vbPXfJaWUmzt0pIE31+zlkoGx9HThJWNPxp4z8BjgWxHZAqwDPjfGLAYeBiaKSBYwwfbYY0wZEkdiZDDPLN+jGz4o5YHeWJ1HVV2j2559A/id6gXGmBxgSCvPlwHjHRHKFfj5+nDz2J7c99F2VmeXMbpXF6sjKaXaSU1dI69+l8u4ftGcFRdudZzTpldinsRlwxKICQvk6WVZVkdRSrWjd9bmU1HbwC1ufPYNWuAnFeTvy01jerI2t5y1ObrtmlKe4Eh9Ey+uzGZ0r84M7xZpdZwzogV+ClePSKJLSCBP6Vm4Uh7h7bV7Ka2u57bxfayOcsa0wE8hyN+Xm8f2YHV2Gevzyq2Oo5Q6A0cbmnhxZQ6jenQmrbt7n32DFrhdZo3oRpeQAB0LV8rNzV+XT0lVHbdN6G11lHahBW6H4ABf5o7pwaqsUjbsrbA6jlLqNBxtaOKFb7JJ6x7JyB6drY7TLrTA7XTNyG5EdgzQsXCl3NS76wsoOlzH78Z7xtk3aIHbrUOAH788rwcrd5ewKV/PwpVyJ3WNTTy/IpuzkzsxqqdnnH2DFnibXDuqG506+PPkV3oWrpQ7WbCugIOHj3Lb+D6IuP5mxfbSAm+DjoF+3DS2Jyt3l5ChM1KUcgtHG5p4dvke0pIjGd3Lc86+QQu8za4d1TIj5Ymlu62OopSyw1vf76W4qo47LvSss2/QAm+zDgF+/Or8XqzOLmNNtl6dqZQrq61v5IVvWq669JSZJ8fTAj8Ns0YkERMWyBNLd+lKhUq5sPTVLVdd3jGxr9VRHEIL/DQE+fvymwt6sT6vglVZpVbHUUq1oupoAy+uzOb8vlEM79bJ6jgOoQV+mn5+diLxEcH8c+luPQtXygW99l0elbUN3DHR/dc8OREt8NMU6OfLreN6saWgkmWZumuPUq7kUG0DL63KYUL/GAYnRFgdx2G0wM/AZcMT6N6lI49/uUv3zlTKhTz/TTbVdY3ceaHnnn2DFvgZ8ff14faJfdh5sIpPtuy3Oo5SCig+fJTXV+cydUgc/WPDrI7jUHYXuIj4isgmEfnM9ri7iKwVkT0i8q6IBDgupuuaPCiWAbFhPLF0N/WNzVbHUcrrPf11Fo1Nhts9eOz7mLacgd8GZB73+BHgSWNML6ACuLE9g7kLHx/hrov6kl9ey7sZBVbHUcqr7S2rYcG6AmamJdKtc0er4zicXQUuIgnAJOBl22MBxgEf2F6SDkxzREB3cH6fKNKSI3l6WRa19Y1Wx1HKaz2xdDd+vsJvx3nOioMnY+8Z+L+Au4FjYwSdgUpjzLG2KgTiW/tCEZkrIhkiklFSUnJGYV2ViHD3RX0pqarj9dV5VsdRyitlHjjMJ1v2c/3o7kSHBVkdxylOWeAiMhkoNsZsOJ0DGGPmGWNSjTGpUVFRp/Mt3EJqciTj+kXzwopsDtU2WB1HKa/z+JJdhAb6cfMY995pvi3sOQMfDUwRkTxgAS1DJ08BESLiZ3tNArDPIQndyF0/60tVXSPPrdhjdRSlvMr3OWUs21nMzef3JLyDv9VxnOaUBW6MudcYk2CMSQZmAl8bY2YBy4HLbS+bA3zssJRuon9sGJcNS+C11XkUVtRaHUcpr2CM4aFFmcSGB3HD6O5Wx3GqM5kH/gfgDhHZQ8uY+CvtE8m93TGxDwI88aUuN6uUM3y+7QBbCg9x54V9CfL3tTqOU7WpwI0xK4wxk233c4wxacaYXsaYK4wxdY6J6F7iIoK54dzufLR5H9v3HbI6jlIerb6xmUcX76Jf11CmD211HoVH0ysxHeBX5/ckItifh7/YqQtdKeVAb32/l/zyWu65uB++Pp61WYM9tMAdICzIn1vH9ebbPaWs1OVmlXKIQ0ca+L+vsxjdqzNj+3juDLeT0QJ3kGtGdiMpsgMPLcqkSRe6UqrdvfBNNhW1Ddx7cX+P2yrNXlrgDhLg58PdF/Vl58EqPtigl9gr1Z4Kymt55dtcpqXEMTA+3Oo4ltECd6BJg2IZ3q0Tjy3ZTXWdXmKvVHt5ZPFOfATuvqif1VEspQXuQCLCnycPoLS6jueW68U9SrWHjLxyPtt6gLljehIXEWx1HEtpgTvYkMQIpg+N5+Vvcyko14t7lDoTzc2GBz77gZiwQG4e28PqOJbTAneCuy/qi4+0/NqnlDp9H2/Zx5bCQ9z1s350CPA79Rd4OC1wJ4gND2bumJ58tvUAG/aWWx1HKbd0pL6JRxfvYlB8ODO88KKd1miBO8nNY3sQExbI3z79QffPVOo0vLgymwOHjvKnyQPw8cKLdlqjBe4kHQL8uOfifmwpPMQHGwutjqOUWymsqOX5FdlMGhRLWvdIq+O4DC1wJ5qWEs+wpAgeXbyTw0d1zXCl7PWPRZmIwP+b1N/qKC5FC9yJRIS/TR1IWU09T32VZXUcpdzCd3tKWbTtILec34t4L582+FNa4E42MD6cmWcnkb46j6yiKqvjKOXSGpqa+eunO0iMDOaXY3Ta4E9pgVvg9xf2oUOAL/d/ukNXK1TqJN5cs5fdRdX8adIAr1vr2x5a4BboHBLInRf25bs9ZSzZcdDqOEq5pNLqOp78ajdj+kQxcUCM1XFckha4RWaNSKJf11D+9ukP1NbrOilK/dTDX+zkSH0Tf548wGtXGzwVe3alDxKRdSKyRUR2iMhfbc93F5G1IrJHRN4VkQDHx/Ucfr4+PDBtIPsPHeXpZbpOilLHW5dbzgcbCvnlmB70ig6xOo7LsucMvA4YZ4wZAqQAF4nISOAR4EljTC+gArjRcTE909nJkVwxPIGXV+XoG5pK2TQ0NfOnf28nPiKY347rbXUcl2bPrvTGGFNte+hv+zDAOOAD2/PpwDSHJPRw917Sn5AgP/747+36hqZSwKvf5rKrqIr7p5xFcIC+cXkydo2Bi4iviGwGioGlQDZQaYw5NnhbCLS6OIGIzBWRDBHJKCkpaY/MHiWyYwB/uKgfa3PL+WjTPqvjKGWp/ZVH+NdXWUzoH6NvXNrBrgI3xjQZY1KABCANsHsVdWPMPGNMqjEmNSrKO/etO5UrUxMZlhTBg59ncqhWr9BU3uuvn+7AYPjLpQOsjuIW2jQLxRhTCSwHRgERInJsPccEQE8fT5OPj/D3aYOoqK3nkSW65KzyTssyi1iyo4jfju9NYmQHq+O4BXtmoUSJSITtfjAwEcikpcgvt71sDvCxo0J6gwFxYdx4bnfeWZvPulxdclZ5l+q6Rv747+30iQnhF+fqFZf2sucMPBZYLiJbgfXAUmPMZ8AfgDtEZA/QGXjFcTG9w+0T+5DQKZh7F26lrrHJ6jhKOc3jS3Zx8PBRHpoxmAA/vTzFXvbMQtlqjBlqjBlsjBlojPmb7fkcY0yaMaaXMeYKY0yd4+N6tg4Bfjw4fRDZJTU8uzzb6jhKOcXG/ArS1+Qxe2Q3hnfrZHUct6L/1LmYsX2imJYSx/Mr9rBb54YrD1ff2My9H24jJjSIu37W1+o4bkcL3AX9afIAQgL9uHfhNt29R3m0l1blsKuoigemDSQ0yN/qOG5HC9wFdQ4J5I+TBrBhbwVvfr/X6jhKOUR2STVPLcvikkFddc73adICd1EzhsUzpk8UjyzeSX5ZrdVxlGpXTc2Gu97fQrC/L/dfepbVcdyWFriLEhEemjEIHxH+8OFWHUpRHuW173LZmF/JX6ecRXRYkNVx3JYWuAuLjwjmvkn9WZNTxjvr8q2Oo1S7yC2t4bElu5jQP4apKXFWx3FrWuAububZiZzbqwsPLcqkoFyHUpR7OzZ0Eujnwz+mD9R1vs+QFriLExEevmwQAPcs3KorFiq3lr46j4y9FdyvQyftQgvcDSR06sD/m9Sf7/aU8dZaHUpR7imnpJpHl+xkXL9opg9tdfFS1UZa4G7i6rQkzuvdhX98nkluaY3VcZRqk8amZm5/bwtB/r48PGOQDp20Ey1wNyEiPHb5EAL8fLj93c00NjVbHUkpuz27PJstBZU8OG2QDp20Iy1wN9I1PIi/TxvI5oJKnluha6Uo97CloJKnv85i+tB4Jg2OtTqOR9ECdzOXDoljakocTy/LYmthpdVxlDqpI/VN3P7eZqJDA7l/il6w0960wN3Q36YMpEtIILe/u5kj9brsrHJdD3+RSU5JDY9fMYTwYF3rpL1pgbuh8A7+/PPnQ8guqeGBz3+wOo5SrVqWWUT6mr3cMLo7o3t1sTqOR9ICd1Oje3XhprE9eGdtPou3H7A6jlL/pejwUe76YCsDYsP4w8W6TKyjaIG7sTsn9mVwQjh3f7CVfZVHrI6jFNByteWx4b2nrxpKoJ+v1ZE8lha4Gwvw8+HpmUNbfmAW6NRC5RpeXJnN6uwy7p8ygF7RIVbH8Wj2bGqcKCLLReQHEdkhIrfZno8UkaUikmW71b2QLJDcpSMPTBvIurxynlm+x+o4ysttyq/gn1/uZtLgWH6emmh1HI9nzxl4I3CnMWYAMBK4RUQGAPcAy4wxvYFltsfKAjOGJTB9aDxPL8tidXap1XGUlzpU28Bv3tlE17Ag/jFdr7Z0Bns2NT5gjNlou18FZALxwFQg3faydGCao0KqU3tg2kCSu3Tkt/M3U3z4qNVxlJdpbjbc+f5miquO8uysYTpl0EnaNAYuIsnAUGAtEGOMOTb94SDQ6p5IIjJXRDJEJKOkpOQMoqqTCQn04/lZw6mua+DW+Zt0PFw51bxVOXyVWcx9l/QnJTHC6jhew+4CF5EQ4EPgd8aYw8d/zrSscdrqOqfGmHnGmFRjTGpUVNQZhVUn17drKH+fNoi1ueU8+dVuq+MoL7E2p4zHluxi0qBY5pyTbHUcr2JXgYuIPy3l/bYxZqHt6SIRibV9PhYodkxE1RaXD0/gytREnl2ezfKd+r9EOVZJVR23zt9EYqdgHr5Mx72dzZ5ZKAK8AmQaY5447lOfAHNs9+cAH7d/PHU6/jr1LPp1DeV3727WDZGVwzQ0NXPr/I0cOtLAc7OGExqk497OZs8Z+GhgNjBORDbbPi4BHgYmikgWMMH2WLmAIH9fXpw9HGMMc9/MoLa+0epIygM9tGgn3+eU84/pgxgQF2Z1HK9kzyyUb40xYowZbIxJsX0sMsaUGWPGG2N6G2MmGGPKnRFY2adb5448fdVQdhVVcdcHuhWbal8LNxby6ne5XHdOMpcNT7A6jtfSKzE92Pl9o7nrZ335fOsBXlyZY3Uc5SG27zvEvQu3MaJ7JPdN6m91HK+mBe7hfjW2J5MGxfLo4p2s3K3TONWZKauu46Y3N9C5YwDPzhqGv69WiJX0T9/DiQiPXj6YPjGh3PLORvYUV1sdSbmpusYmbn5rAyXVdbwwezhdQgKtjuT1tMC9QMdAP166NpUAXx9uTF9PRU291ZGUmzHGcO/CbazPq+CfVwxhcIJerOMKtMC9RGJkB+ZdO5wDlUe56a0N1DfqlZrKfs+tyGbhxn3cPqEPlw6JszqOstEC9yLDu0Xy6OWDWZdbzn0fbdOZKcouX2w7wGNLdjFlSBy/Hd/L6jjqOH5WB1DONW1oPDkl1Tz99R66R3Xk1+frD6Q6sc0Fldz+3maGJUXw6OWD9UpLF6MF7oV+N6EPuWW1PLp4F7HhQUwfqvN41f/KK63hhtfXExUayIuzUwny1511XI0WuBfy8REev2IwJVVHuev9rXQJCeS83rrQmPqPkqo6rn11HcYY0q9PIypUZ5y4Ih0D91KBfr68ODuVXtEh3PzmBrbvO2R1JOUiauoauTF9PcVVR3nlurPpEaXborkqLXAvFh7sz+vXpxEe7M/1r6+noFwXvvJ2DU3N3PLORrbvO8QzVw1jWJLulOjKtMC9XNfwINJvSKO+sZlZL6+lSHfz8VpNzYY73tvCil0lPDh9EBMGtLpHi3IhWuCK3jGhvH792ZRV13HNy2sp1wt9vI4xhvs+2sanW/Zzz8X9uCotyepIyg5a4AqAoUmdeHnO2eSX1zLn1XVUHW2wOpJyEmMMD36eyYL1Bfzmgl7cPLan1ZGUnbTA1Y9G9ezM89cMI/PAYW58XdcR9xZPLcvi5W9bloa988I+VsdRbaAFrv7LuH4x/GtmChl7y7nh9fVa4h7u6WVZ/OurLC4fnsCfJw/QC3XcjBa4+h+TB8fx5JUprMvVEvdkT32VxRNLdzNjWDyPXDYYHx8tb3djz56Yr4pIsYhsP+65SBFZKiJZtluda+RhpqbE/1ji1722npo6LXFP8uTS3Tz51W4uG5bAY5cPwVfL2y3Zcwb+OnDRT567B1hmjOkNLLM9Vh5mako8/5o5lIy8cq5/bb2+sekBjDE88eUunlqWxRXDE3j08sFa3m7Mnj0xVwI/3e9yKpBuu58OTGvnXMpFTBkSx1Mzh7Ihv4JZOsXQrTU3G/766Q88/fUerkxN5JHLtLzd3emOgccYYw7Y7h8ETjjjX0TmikiGiGSUlOiWXu7o0iFxzJs9nF0Hq7jihdUcOHTE6kiqjRqamvn9+1t4fXUevzi3Ow/NGKRj3h7gjN/ENC2LSp9wYWljzDxjTKoxJjUqShdMclfj+8fwxg1pFB+u4/Ln15BToluzuYujDU386q2NLNy0j99f2If7JvXX8vYQp1vgRSISC2C7LW6/SMpVjejRmflzR3K0oYkrXljDpvwKqyOpU6isrefaV9axbGcRD0w9i9+M661TBT3I6Rb4J8Ac2/05wMftE0e5uoHx4bx/8yg6Bvoxc973LN5+4NRfpCyxt6yGGc+tZnNhJU/PHMrsUclWR1LtzJ5phPOBNSJVuC0AAArZSURBVEBfESkUkRuBh4GJIpIFTLA9Vl6iR1QIH/36HAbEhfGrtzfy8qoc3Z7NxWzYW8H051ZTUVvPO78YoftYeqhTbuhgjLnqBJ8a385ZlBvpHBLI/F+O5I73NvP3zzPJK6vhL5eehb+vXhtmtU+37Of3728hNjyI165Po3uXjlZHUg6iP23qtAX5+/LMVcO4aWwP3vo+n1kvraWkqs7qWF6rqdnw0BeZ3Dp/E4MTwln469Fa3h5OC1ydER8f4d6L+/PUzBS27qtkyjPfsqWg0upYXqeytp7rXlvHi9/kcM3IJN7+xUgiOwZYHUs5mBa4ahdTU+L54OZz8BHhihfX8N76Ah0Xd5Id+w8x5ZnvWJtTziOXDeLv0wYR4Kc/2t5A/y+rdjMwPpxPbz2Xs5M7cfeHW7n93c1U6xoqDmOMIX11HtOfXU1dYxMLbhrJlWfrRgzeRHelV+0qsmMAb9wwgmeX7+FfX+1mS+Eh/u+qoQyMD7c6mkc5VNvA3R9uYcmOIsb1i+bxK4bokIkX0jNw1e58fYTfju/NgrmjOFLfxIznVvPSyhyamnVIpT2szi7lkqdX8fXOYv44qT8vX5uq5e2ltMCVw6R1j+SL285jbN8oHlyUyZUvriG3tMbqWG6rtr6Rv3y8natfWou/r/D+zefwi/N66GXxXkwLXDlUp44BzJs9nCd+PoRdRVVc/NRKXv8ul2Y9G2+T9XnlXPzUKtLX7OW6c5JZdNt5pCRGWB1LWUzHwJXDiQgzhiVwTs8u3LNwK/d/+gMfb9nPA1MH6tj4KVTU1PPI4p0sWF9AYmQwC+aOZGSPzlbHUi5CnDnVKzU11WRkZDjteMr1GGNYuHEf/1iUSUVtPdeOSuaOC/sQFuRvdTSX0txseH9DAQ9/sZPDRxu5YXQyv5vQh46Bes7ljURkgzEm9afP698G5VQiwmXDE5jQP4bHv9xF+po8Pt92gDsn9uHy4Qn46aX4ZOSV8+CiTDblV3J2cicemDaQfl3DrI6lXJCegStLbS2s5C+f7GBTfiW9o0O45+J+jOsX7ZVLnmaXVPPo4p0s2VFEdGggd/2sL5cPT/DKPwv13050Bq4FrixnjGHJjoM8ungXOaU1pHWP5LbxvTmnZ2evKK/8slqe/2YP72UUEuzvy01jenDjed3pEKC/IKsWWuDK5TU0NbNgfQHPfJ1F0eE6UhIjuHVcL489I88qquK5Fdl8smU/vj7CVWcncuv43nQJCbQ6mnIxWuDKbdQ1NvHBhkKeX5FNYcUR+saEcu053ZiWEu/2b+I1NxtW7SnlzTV5LNtZTJCfL9eMTOKX5/UgOizI6njKRWmBK7fT0NTMJ5v388q3ufxw4DChgX5cNjyBq0ck0Scm1Op4bVJeU8/CjYW89f1e8spq6RISwNVpSVw3urteRalOSQtcuS1jDBvzK3lzTR6Lth2kvqmZ/rFhTEuJ49IhccRFBFsdsVU1dY18lVnEx5v3s3J3CY3NhtRunZg9qhsXD4zVFQOV3bTAlUcora7jsy37+ffm/Wy2rTs+NCmCC/pGc37fKAbGhVt6afm+yiOs2FXM8p0lfLenlCMNTcSFBzElJZ5pQ+N0OqA6LQ4pcBG5CHgK8AVeNsacdG9MLXDVnvaW1fDJ5v18tbOYrYWVGANdQgIY0b0zw7p1YlhSBGfFhTvsTNcYQ25pDRvzK9mYX8H63HKyiqsBiI8IZly/aC4dEkdqt066Xok6I+1e4CLiC+wGJgKFwHrgKmPMDyf6Gi1w5Sil1XWs3F3CN7tLyMirYF/lEQAC/HzoGRVCr+gQekWF0DO6I13DgogKDSQ6NIjgAN+Tft+GpmbKqusprjpK8eE68spq2FNczZ7iarKKqzl0pAGA0EA/UpIiGNM7igv6RdEzKsQjZ84oazjiSsw0YI8xJsd2gAXAVOCEBa6Uo3QJCWTGsARmDEsA4OCho2zMr2BzQSW7i6rYlF/Bp1v2/8/XBfv7EuTvQ6CfL4H+PviIUNfQRF1jM3WNza1uSBHZMYBeUSFcMiiWwQnhDEvqRK/oEHz1LFs52ZkUeDxQcNzjQmDET18kInOBuQBJSbpbiHKOruFBXDIolksGxf743JH6JvLKaiiuqqP48FFKqusor663lXVLaTc1GwL9/lPqoUF+RIcFEhUSSHRYEImdgums87SVi3D4pFpjzDxgHrQMoTj6eEqdSHCAL/1jw+gfe+rXKuUOzuTdnX1A4nGPE2zPKaWUcoIzKfD1QG8R6S4iAcBM4JP2iaWUUupUTnsIxRjTKCK/AZbQMo3wVWPMjnZLppRS6qTOaAzcGLMIWNROWZRSSrWBXsurlFJuSgtcKaXclBa4Ukq5KS1wpZRyU05djVBESoC9p/nlXYDSdozTnlw1m6vmAtfN5qq5wHWzuWoucN1sbc3VzRgT9dMnnVrgZ0JEMlpbzMUVuGo2V80FrpvNVXOB62Zz1VzgutnaK5cOoSillJvSAldKKTflTgU+z+oAJ+Gq2Vw1F7huNlfNBa6bzVVzgetma5dcbjMGrpRS6r+50xm4Ukqp42iBK6WUm3KrAheRB0Rkq4hsFpEvRSTO6kwAIvKYiOy0ZftIRCKsznSMiFwhIjtEpFlELJ9OJSIXicguEdkjIvdYnecYEXlVRIpFZLvVWY4nIokislxEfrD9f7zN6kzHiEiQiKwTkS22bH+1OtPxRMRXRDaJyGdWZzmeiOSJyDZbj53RJsFuVeDAY8aYwcaYFOAz4M9WB7JZCgw0xgymZaPney3Oc7ztwAxgpdVBbBthPwtcDAwArhKRAdam+tHrwEVWh2hFI3CnMWYAMBK4xYX+zOqAccaYIUAKcJGIjLQ40/FuAzKtDnECFxhjUs50LrhbFbgx5vBxDzsCLvEOrDHmS2PMsd1vv6dldyKXYIzJNMbssjqHzY8bYRtj6oFjG2FbzhizEii3OsdPGWMOGGM22u5X0VJI8damamFaVNse+ts+XOJnUkQSgEnAy1ZncSS3KnAAEXlQRAqAWbjOGfjxbgC+sDqEi2ptI2yXKCN3ICLJwFBgrbVJ/sM2TLEZKAaWGmNcJdu/gLuBZquDtMIAX4rIBtum76fN5QpcRL4Ske2tfEwFMMbcZ4xJBN4GfuMquWyvuY+WX3nfdlYue7Mp9yYiIcCHwO9+8puopYwxTbYhzQQgTUQGWp1JRCYDxcaYDVZnOYFzjTHDaBlKvEVExpzuN3L4rvRtZYyZYOdL36ZlN6C/ODDOj06VS0SuAyYD442TJ9e34c/MaroR9mkQEX9ayvttY8xCq/O0xhhTKSLLaXkfweo3gkcDU0TkEiAICBORt4wx11icCwBjzD7bbbGIfETL0OJpvUflcmfgJyMivY97OBXYaVWW44nIRbT8ujbFGFNrdR4Xphtht5GICPAKkGmMecLqPMcTkahjM65EJBiYiAv8TBpj7jXGJBhjkmn5O/a1q5S3iHQUkdBj94ELOYN/8NyqwIGHbUMDW2n5D3eVKVXPAKHAUtvUoBesDnSMiEwXkUJgFPC5iCyxKovtjd5jG2FnAu+5ykbYIjIfWAP0FZFCEbnR6kw2o4HZwDjb363NtjNLVxALLLf9PK6nZQzcpabsuaAY4FsR2QKsAz43xiw+3W+ml9IrpZSbcrczcKWUUjZa4Eop5aa0wJVSyk1pgSullJvSAldKKTelBa6UUm5KC1wppdzU/wfMjZcCPi6P6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = X * W\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - Y))\n",
    "\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean(2 * (W * X - Y) * X)  # 미분식\n",
    "descent = W - learning_rate * gradient  # 새로운 W값\n",
    "update = W.assign(descent)  # W에 descent를 assign하는 operation을 update라고 칭하겠다.\n",
    "# W = W - learning_rate * gradient 라고 쓰고싶겠지만 tensor는 그렇게 작동 못한다.\n",
    "# 새로운 W를 descent에 할당하고 그걸 새로 업데이트 해주는 tensor를 만들어야한다.\n",
    "# 물론 update = W.assign(W - learning_rate * gradient) 이렇게 써도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\t0.36719\t0.83805\n",
      " 1\t0.00163\t0.98920\n",
      " 2\t0.00001\t0.99928\n",
      " 3\t0.00000\t0.99995\n",
      " 4\t0.00000\t1.00000\n",
      " 5\t0.00000\t1.00000\n",
      " 6\t0.00000\t1.00000\n",
      " 7\t0.00000\t1.00000\n",
      " 8\t0.00000\t1.00000\n",
      " 9\t0.00000\t1.00000\n",
      "10\t0.00000\t1.00000\n",
      "11\t0.00000\t1.00000\n",
      "12\t0.00000\t1.00000\n",
      "13\t0.00000\t1.00000\n",
      "14\t0.00000\t1.00000\n",
      "15\t0.00000\t1.00000\n",
      "16\t0.00000\t1.00000\n",
      "17\t0.00000\t1.00000\n",
      "18\t0.00000\t1.00000\n",
      "19\t0.00000\t1.00000\n",
      "20\t0.00000\t1.00000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict = {X: x_data, Y: y_data})\n",
    "    print('{:2d}\\t{:.5f}\\t{:.5f}'.format(step, sess.run(cost, feed_dict = {X: x_data, Y: y_data}), *sess.run(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\t0.86683\t0.54201\n",
      "50\t0.93443\t0.14906\n",
      "100\t0.98058\t0.04416\n",
      "150\t0.99425\t0.01308\n",
      "200\t0.99830\t0.00387\n",
      "250\t0.99950\t0.00115\n",
      "300\t0.99985\t0.00034\n",
      "350\t0.99996\t0.00010\n",
      "400\t0.99999\t0.00003\n",
      "450\t1.00000\t0.00001\n",
      "500\t1.00000\t0.00000\n",
      "550\t1.00000\t0.00000\n",
      "600\t1.00000\t0.00000\n",
      "650\t1.00000\t0.00000\n",
      "700\t1.00000\t0.00000\n",
      "750\t1.00000\t0.00000\n",
      "800\t1.00000\t0.00000\n",
      "850\t1.00000\t0.00000\n",
      "900\t1.00000\t0.00000\n",
      "950\t1.00000\t0.00000\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "X = tf.placeholder(tf.float32, shape = None)\n",
    "y = tf.placeholder(tf.float32, shape = None)\n",
    "\n",
    "x_data = [1, 2, 3]; y_data = [1, 2, 3]\n",
    "learning_rate = 0.1\n",
    "\n",
    "gradient_W = tf.reduce_mean(2 * (W * X + b - y) * X)\n",
    "gradient_b = tf.reduce_mean(2 * (W * X + b - y))\n",
    "\n",
    "update_W = W.assign(W - learning_rate * gradient_W)\n",
    "update_b = b.assign(b - learning_rate * gradient_b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        W_val, b_val, _, _ = sess.run([W, b, update_W, update_b], feed_dict = {X: x_data, y: y_data})\n",
    "        if step % 50 == 0:\n",
    "            print('{:2d}\\t{:.5f}\\t{:.5f}'.format(step, *W_val, *b_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0\t1.12689\t-0.13409\n",
      " 50\t1.02073\t-0.04714\n",
      "100\t1.00614\t-0.01396\n",
      "150\t1.00182\t-0.00414\n",
      "200\t1.00054\t-0.00123\n",
      "250\t1.00016\t-0.00036\n",
      "300\t1.00005\t-0.00011\n",
      "350\t1.00001\t-0.00003\n",
      "400\t1.00000\t-0.00001\n",
      "450\t1.00000\t-0.00000\n",
      "500\t1.00000\t-0.00000\n",
      "550\t1.00000\t-0.00000\n",
      "600\t1.00000\t-0.00000\n",
      "650\t1.00000\t-0.00000\n",
      "700\t1.00000\t-0.00000\n",
      "750\t1.00000\t-0.00000\n",
      "800\t1.00000\t-0.00000\n",
      "850\t1.00000\t-0.00000\n",
      "900\t1.00000\t-0.00000\n",
      "950\t1.00000\t-0.00000\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_normal([1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(tf.reduce_mean(tf.square(W * X + b - y)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        _, W_val, b_val = sess.run([train, W, b], feed_dict = {X: [1, 2, 3], y: [1, 2, 3]})\n",
    "        if step % 50 == 0:\n",
    "            print('{:3d}\\t{:.5f}\\t{:.5f}'.format(step, *W_val, *b_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 04-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 91., 98., 66.]\n",
    "x3_data = [75., 93., 90., 100., 70.]\n",
    "\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([1]))\n",
    "w2 = tf.Variable(tf.random_normal([1]))\n",
    "w3 = tf.Variable(tf.random_normal([1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [71.90421 79.75805 82.25426 87.56616 60.68464] 9083.104\n",
      "100 [156.29443 181.42369 182.30484 196.53426 138.2755 ] 10.140356\n",
      "200 [156.17319 181.50665 182.26749 196.50876 138.38307] 9.620293\n",
      "300 [156.05519 181.58737 182.23112 196.48396 138.48778] 9.127689\n",
      "400 [155.94032 181.66599 182.19574 196.45985 138.58966] 8.660991\n",
      "500 [155.8285  181.74248 182.16129 196.43642 138.68878] 8.218939\n",
      "600 [155.71968 181.81694 182.12775 196.41367 138.78523] 7.8002105\n",
      "700 [155.61374 181.8894  182.09508 196.39153 138.8791 ] 7.4035096\n",
      "800 [155.51062 181.95996 182.0633  196.37003 138.97044] 7.0277205\n",
      "900 [155.41022 182.02864 182.03233 196.34912 139.05931] 6.6716948\n",
      "1000 [155.3125  182.09547 182.0022  196.3288  139.14577] 6.3344946\n",
      "1100 [155.21738 182.16052 181.97285 196.30905 139.22992] 6.0150323\n",
      "1200 [155.12476 182.22386 181.94427 196.28986 139.31177] 5.7123713\n",
      "1300 [155.03464 182.28552 181.91647 196.27122 139.39145] 5.425677\n",
      "1400 [154.94687 182.34552 181.8894  196.2531  139.46895] 5.154086\n",
      "1500 [154.86147 182.40393 181.86304 196.2355  139.54437] 4.896807\n",
      "1600 [154.77832 182.46078 181.83736 196.21838 139.61775] 4.653071\n",
      "1700 [154.69739 182.51611 181.81236 196.20178 139.68915] 4.4222007\n",
      "1800 [154.61858 182.56999 181.78802 196.18564 139.7586 ] 4.2034445\n",
      "1900 [154.54189 182.62245 181.76433 196.16997 139.8262 ] 3.9962108\n",
      "2000 [154.46722 182.67348 181.74127 196.15471 139.89198] 3.7999203\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        _, hy_val, cost_val = sess.run([train, hypothesis, cost], \n",
    "                                       feed_dict = {x1: x1_data, x2: x2_data, x3: x3_data, y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, hy_val, cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[-39.4316  ]\n",
      " [-62.887066]\n",
      " [-53.819584]\n",
      " [-58.30785 ]\n",
      " [-52.47447 ]]\n",
      "51051.695\n",
      "\n",
      "100\n",
      "[[160.66435]\n",
      " [178.1394 ]\n",
      " [183.39468]\n",
      " [200.03809]\n",
      " [131.47777]]\n",
      "52.13723\n",
      "\n",
      "200\n",
      "[[160.39345]\n",
      " [178.32568]\n",
      " [183.31238]\n",
      " [199.97318]\n",
      " [131.72676]]\n",
      "49.45878\n",
      "\n",
      "300\n",
      "[[160.12982]\n",
      " [178.50691]\n",
      " [183.23228]\n",
      " [199.90987]\n",
      " [131.96916]]\n",
      "46.921326\n",
      "\n",
      "400\n",
      "[[159.87332]\n",
      " [178.68332]\n",
      " [183.15439]\n",
      " [199.84816]\n",
      " [132.20515]]\n",
      "44.517433\n",
      "\n",
      "500\n",
      "[[159.62375]\n",
      " [178.85495]\n",
      " [183.07858]\n",
      " [199.788  ]\n",
      " [132.43492]]\n",
      "42.240093\n",
      "\n",
      "600\n",
      "[[159.38089]\n",
      " [179.022  ]\n",
      " [183.00485]\n",
      " [199.72934]\n",
      " [132.6586 ]]\n",
      "40.08257\n",
      "\n",
      "700\n",
      "[[159.14458]\n",
      " [179.18451]\n",
      " [182.93314]\n",
      " [199.67213]\n",
      " [132.87639]]\n",
      "38.038612\n",
      "\n",
      "800\n",
      "[[158.91464]\n",
      " [179.34268]\n",
      " [182.86337]\n",
      " [199.6164 ]\n",
      " [133.08841]]\n",
      "36.102238\n",
      "\n",
      "900\n",
      "[[158.69092]\n",
      " [179.49658]\n",
      " [182.79547]\n",
      " [199.562  ]\n",
      " [133.29483]]\n",
      "34.26769\n",
      "\n",
      "1000\n",
      "[[158.47324]\n",
      " [179.64635]\n",
      " [182.72945]\n",
      " [199.50899]\n",
      " [133.49583]]\n",
      "32.529617\n",
      "\n",
      "1100\n",
      "[[158.26143]\n",
      " [179.79208]\n",
      " [182.66524]\n",
      " [199.45726]\n",
      " [133.69151]]\n",
      "30.882996\n",
      "\n",
      "1200\n",
      "[[158.05534]\n",
      " [179.9339 ]\n",
      " [182.60275]\n",
      " [199.40683]\n",
      " [133.88205]]\n",
      "29.3229\n",
      "\n",
      "1300\n",
      "[[157.85484]\n",
      " [180.0719 ]\n",
      " [182.54199]\n",
      " [199.35765]\n",
      " [134.06754]]\n",
      "27.844982\n",
      "\n",
      "1400\n",
      "[[157.65973]\n",
      " [180.20615]\n",
      " [182.48288]\n",
      " [199.30968]\n",
      " [134.24815]]\n",
      "26.444668\n",
      "\n",
      "1500\n",
      "[[157.46991]\n",
      " [180.33684]\n",
      " [182.42538]\n",
      " [199.2629 ]\n",
      " [134.42403]]\n",
      "25.117868\n",
      "\n",
      "1600\n",
      "[[157.28522]\n",
      " [180.46397]\n",
      " [182.36946]\n",
      " [199.21725]\n",
      " [134.59523]]\n",
      "23.860947\n",
      "\n",
      "1700\n",
      "[[157.10551]\n",
      " [180.58768]\n",
      " [182.31505]\n",
      " [199.17273]\n",
      " [134.76195]]\n",
      "22.669987\n",
      "\n",
      "1800\n",
      "[[156.93066]\n",
      " [180.70805]\n",
      " [182.26215]\n",
      " [199.1293 ]\n",
      " [134.92427]]\n",
      "21.541607\n",
      "\n",
      "1900\n",
      "[[156.76059]\n",
      " [180.8252 ]\n",
      " [182.2107 ]\n",
      " [199.08693]\n",
      " [135.08234]]\n",
      "20.472515\n",
      "\n",
      "2000\n",
      "[[156.59508]\n",
      " [180.93916]\n",
      " [182.16064]\n",
      " [199.04561]\n",
      " [135.23625]]\n",
      "19.459507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[73., 80., 75.],\n",
    "          [93., 88., 93.],\n",
    "          [89., 91., 90.],\n",
    "          [96., 98., 100.],\n",
    "          [73., 66., 70.]]\n",
    "y_data = [[152.],\n",
    "          [185.],\n",
    "          [180.],\n",
    "          [196.],\n",
    "          [142.]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "# 데이터가 몇개 들어올지는 모르고 feature는 3개다.\n",
    "# [row, col]이 들어온다고 생각. 이때 데이터가 몇개 들어올지는 모르니까 None\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 1e-5).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        _, hy_val, cost_val = sess.run([train, hypothesis, cost], \n",
    "                                       feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, hy_val, cost_val, sep = '\\n', end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 04-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 73.,  80.,  75., 152.],\n",
       "       [ 93.,  88.,  93., 185.],\n",
       "       [ 89.,  91.,  90., 180.],\n",
       "       [ 96.,  98., 100., 196.],\n",
       "       [ 73.,  66.,  70., 142.],\n",
       "       [ 53.,  46.,  55., 101.],\n",
       "       [ 69.,  74.,  77., 149.],\n",
       "       [ 47.,  56.,  60., 115.],\n",
       "       [ 87.,  79.,  90., 175.],\n",
       "       [ 79.,  70.,  88., 164.],\n",
       "       [ 69.,  70.,  73., 141.],\n",
       "       [ 70.,  65.,  74., 141.],\n",
       "       [ 93.,  95.,  91., 184.],\n",
       "       [ 79.,  80.,  73., 152.],\n",
       "       [ 70.,  73.,  78., 148.],\n",
       "       [ 93.,  89.,  96., 192.],\n",
       "       [ 78.,  75.,  68., 147.],\n",
       "       [ 81.,  90.,  93., 183.],\n",
       "       [ 88.,  92.,  86., 177.],\n",
       "       [ 78.,  83.,  77., 159.],\n",
       "       [ 82.,  86.,  90., 177.],\n",
       "       [ 86.,  82.,  89., 175.],\n",
       "       [ 78.,  83.,  85., 175.],\n",
       "       [ 76.,  83.,  71., 149.],\n",
       "       [ 96.,  93.,  95., 192.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "xy = np.loadtxt('/home/data/data-01-test-score.csv', delimiter = ',', dtype = np.float32)\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3)\n",
      "\n",
      "[[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]\n",
      " [ 69.  74.  77.]\n",
      " [ 47.  56.  60.]\n",
      " [ 87.  79.  90.]\n",
      " [ 79.  70.  88.]\n",
      " [ 69.  70.  73.]\n",
      " [ 70.  65.  74.]\n",
      " [ 93.  95.  91.]\n",
      " [ 79.  80.  73.]\n",
      " [ 70.  73.  78.]\n",
      " [ 93.  89.  96.]\n",
      " [ 78.  75.  68.]\n",
      " [ 81.  90.  93.]\n",
      " [ 88.  92.  86.]\n",
      " [ 78.  83.  77.]\n",
      " [ 82.  86.  90.]\n",
      " [ 86.  82.  89.]\n",
      " [ 78.  83.  85.]\n",
      " [ 76.  83.  71.]\n",
      " [ 96.  93.  95.]]\n",
      "\n",
      "(25, 1)\n",
      "\n",
      "[[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]\n",
      " [149.]\n",
      " [115.]\n",
      " [175.]\n",
      " [164.]\n",
      " [141.]\n",
      " [141.]\n",
      " [184.]\n",
      " [152.]\n",
      " [148.]\n",
      " [192.]\n",
      " [147.]\n",
      " [183.]\n",
      " [177.]\n",
      " [159.]\n",
      " [177.]\n",
      " [175.]\n",
      " [175.]\n",
      " [149.]\n",
      " [192.]]\n"
     ]
    }
   ],
   "source": [
    "x_data = xy[:, :-1]\n",
    "y_data = xy[:, [-1]]\n",
    "# 여기 -1을 [-1] 이렇게 넣지 않으면 Y = tf.placeholder(tf.float32, shape = [None, 1]) 여기 들어가질 않음\n",
    "print(x_data.shape, x_data, y_data.shape, y_data, sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 1e-5).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4889.522\n",
      "[[ 82.896484]\n",
      " [111.57126 ]\n",
      " [103.85621 ]\n",
      " [112.670364]\n",
      " [ 88.33291 ]\n",
      " [ 65.67213 ]\n",
      " [ 80.20309 ]\n",
      " [ 53.3645  ]\n",
      " [106.415924]\n",
      " [ 98.5498  ]\n",
      " [ 81.27839 ]\n",
      " [ 85.1861  ]\n",
      " [108.01663 ]\n",
      " [ 91.21475 ]\n",
      " [ 82.29199 ]\n",
      " [111.67417 ]\n",
      " [ 91.11056 ]\n",
      " [ 93.28185 ]\n",
      " [101.209694]\n",
      " [ 89.145805]\n",
      " [ 95.99137 ]\n",
      " [103.417625]\n",
      " [ 90.632774]\n",
      " [ 85.13391 ]\n",
      " [114.01451 ]]\n",
      "\n",
      "100\n",
      "39.456947\n",
      "[[148.4746 ]\n",
      " [190.12225]\n",
      " [181.38805]\n",
      " [197.15434]\n",
      " [148.13635]\n",
      " [109.80245]\n",
      " [143.58592]\n",
      " [100.56135]\n",
      " [179.82149]\n",
      " [166.6189 ]\n",
      " [142.21739]\n",
      " [145.17268]\n",
      " [188.06558]\n",
      " [157.67467]\n",
      " [145.93124]\n",
      " [191.45363]\n",
      " [154.26581]\n",
      " [169.43864]\n",
      " [177.55884]\n",
      " [157.49748]\n",
      " [170.26295]\n",
      " [177.17117]\n",
      " [161.45445]\n",
      " [151.13332]\n",
      " [195.4437 ]]\n",
      "\n",
      "200\n",
      "37.826675\n",
      "[[148.58084]\n",
      " [189.98831]\n",
      " [181.3872 ]\n",
      " [197.19824]\n",
      " [147.93222]\n",
      " [109.69061]\n",
      " [143.75891]\n",
      " [100.89329]\n",
      " [179.69043]\n",
      " [166.57327]\n",
      " [142.26004]\n",
      " [145.11649]\n",
      " [188.01385]\n",
      " [157.54953]\n",
      " [146.07095]\n",
      " [191.38474]\n",
      " [154.01083]\n",
      " [169.73532]\n",
      " [177.54266]\n",
      " [157.51947]\n",
      " [170.4123 ]\n",
      " [177.10617]\n",
      " [161.6062 ]\n",
      " [151.12433]\n",
      " [195.32462]]\n",
      "\n",
      "300\n",
      "36.276043\n",
      "[[148.68456 ]\n",
      " [189.85762 ]\n",
      " [181.38641 ]\n",
      " [197.24104 ]\n",
      " [147.73306 ]\n",
      " [109.581345]\n",
      " [143.92761 ]\n",
      " [101.216934]\n",
      " [179.56241 ]\n",
      " [166.52838 ]\n",
      " [142.30156 ]\n",
      " [145.06152 ]\n",
      " [187.96352 ]\n",
      " [157.42767 ]\n",
      " [146.2071  ]\n",
      " [191.3174  ]\n",
      " [153.76237 ]\n",
      " [170.0246  ]\n",
      " [177.52705 ]\n",
      " [157.5411  ]\n",
      " [170.55792 ]\n",
      " [177.04268 ]\n",
      " [161.75418 ]\n",
      " [151.11584 ]\n",
      " [195.20848 ]]\n",
      "\n",
      "400\n",
      "34.80102\n",
      "[[148.78587]\n",
      " [189.73007]\n",
      " [181.38574]\n",
      " [197.28279]\n",
      " [147.53879]\n",
      " [109.47463]\n",
      " [144.09212]\n",
      " [101.53254]\n",
      " [179.43736]\n",
      " [166.48425]\n",
      " [142.34206]\n",
      " [145.00777]\n",
      " [187.91455]\n",
      " [157.309  ]\n",
      " [146.33983]\n",
      " [191.25163]\n",
      " [153.52023]\n",
      " [170.30673]\n",
      " [177.512  ]\n",
      " [157.5624 ]\n",
      " [170.69989]\n",
      " [176.98065]\n",
      " [161.89853]\n",
      " [151.1079 ]\n",
      " [195.0952 ]]\n",
      "\n",
      "500\n",
      "33.39798\n",
      "[[148.88483 ]\n",
      " [189.60565 ]\n",
      " [181.38516 ]\n",
      " [197.32353 ]\n",
      " [147.34926 ]\n",
      " [109.370384]\n",
      " [144.25255 ]\n",
      " [101.8403  ]\n",
      " [179.31522 ]\n",
      " [166.44092 ]\n",
      " [142.38153 ]\n",
      " [144.95518 ]\n",
      " [187.86694 ]\n",
      " [157.19347 ]\n",
      " [146.46924 ]\n",
      " [191.18742 ]\n",
      " [153.28426 ]\n",
      " [170.58191 ]\n",
      " [177.49747 ]\n",
      " [157.5833  ]\n",
      " [170.83832 ]\n",
      " [176.92004 ]\n",
      " [162.03929 ]\n",
      " [151.10042 ]\n",
      " [194.98476 ]]\n",
      "\n",
      "600\n",
      "32.063442\n",
      "[[148.98148 ]\n",
      " [189.48424 ]\n",
      " [181.38464 ]\n",
      " [197.36325 ]\n",
      " [147.16437 ]\n",
      " [109.268555]\n",
      " [144.40897 ]\n",
      " [102.14038 ]\n",
      " [179.19592 ]\n",
      " [166.39833 ]\n",
      " [142.42    ]\n",
      " [144.90376 ]\n",
      " [187.82062 ]\n",
      " [157.08095 ]\n",
      " [146.59537 ]\n",
      " [191.12468 ]\n",
      " [153.05428 ]\n",
      " [170.85023 ]\n",
      " [177.48349 ]\n",
      " [157.60384 ]\n",
      " [170.97328 ]\n",
      " [176.86086 ]\n",
      " [162.17659 ]\n",
      " [151.09338 ]\n",
      " [194.87701 ]]\n",
      "\n",
      "700\n",
      "30.793991\n",
      "[[149.07585]\n",
      " [189.36577]\n",
      " [181.3842 ]\n",
      " [197.40201]\n",
      " [146.984  ]\n",
      " [109.16909]\n",
      " [144.56151]\n",
      " [102.43302]\n",
      " [179.07942]\n",
      " [166.35649]\n",
      " [142.45747]\n",
      " [144.85347]\n",
      " [187.77556]\n",
      " [156.97137]\n",
      " [146.7183 ]\n",
      " [191.0634 ]\n",
      " [152.83012]\n",
      " [171.11192]\n",
      " [177.46999]\n",
      " [157.62401]\n",
      " [171.10487]\n",
      " [176.80302]\n",
      " [162.31049]\n",
      " [151.08679]\n",
      " [194.77196]]\n",
      "\n",
      "800\n",
      "29.586548\n",
      "[[149.16801 ]\n",
      " [189.25018 ]\n",
      " [181.3838  ]\n",
      " [197.43979 ]\n",
      " [146.80804 ]\n",
      " [109.07195 ]\n",
      " [144.71025 ]\n",
      " [102.718346]\n",
      " [178.9656  ]\n",
      " [166.31538 ]\n",
      " [142.494   ]\n",
      " [144.80428 ]\n",
      " [187.73172 ]\n",
      " [156.86465 ]\n",
      " [146.83815 ]\n",
      " [191.00356 ]\n",
      " [152.61168 ]\n",
      " [171.36711 ]\n",
      " [177.45695 ]\n",
      " [157.64383 ]\n",
      " [171.23317 ]\n",
      " [176.74652 ]\n",
      " [162.44106 ]\n",
      " [151.08058 ]\n",
      " [194.6695  ]]\n",
      "\n",
      "900\n",
      "28.43799\n",
      "[[149.25801]\n",
      " [189.1374 ]\n",
      " [181.3835 ]\n",
      " [197.47667]\n",
      " [146.6364 ]\n",
      " [108.97708]\n",
      " [144.8553 ]\n",
      " [102.9966 ]\n",
      " [178.85448]\n",
      " [166.27504]\n",
      " [142.52962]\n",
      " [144.7562 ]\n",
      " [187.68909]\n",
      " [156.76073]\n",
      " [146.95499]\n",
      " [190.94511]\n",
      " [152.39879]\n",
      " [171.616  ]\n",
      " [177.44438]\n",
      " [157.6633 ]\n",
      " [171.35829]\n",
      " [176.69135]\n",
      " [162.56842]\n",
      " [151.0748 ]\n",
      " [194.5696 ]]\n",
      "\n",
      "1000\n",
      "27.345503\n",
      "[[149.34592 ]\n",
      " [189.02737 ]\n",
      " [181.38324 ]\n",
      " [197.51265 ]\n",
      " [146.46896 ]\n",
      " [108.88443 ]\n",
      " [144.99673 ]\n",
      " [103.267914]\n",
      " [178.74594 ]\n",
      " [166.23541 ]\n",
      " [142.56433 ]\n",
      " [144.70917 ]\n",
      " [187.64758 ]\n",
      " [156.65952 ]\n",
      " [147.06888 ]\n",
      " [190.88803 ]\n",
      " [152.1913  ]\n",
      " [171.8587  ]\n",
      " [177.43227 ]\n",
      " [157.6824  ]\n",
      " [171.48027 ]\n",
      " [176.63745 ]\n",
      " [162.69263 ]\n",
      " [151.06935 ]\n",
      " [194.47214 ]]\n",
      "\n",
      "1100\n",
      "26.30629\n",
      "[[149.43173 ]\n",
      " [188.92    ]\n",
      " [181.38303 ]\n",
      " [197.54773 ]\n",
      " [146.3056  ]\n",
      " [108.793915]\n",
      " [145.13466 ]\n",
      " [103.532486]\n",
      " [178.63995 ]\n",
      " [166.19652 ]\n",
      " [142.59814 ]\n",
      " [144.66318 ]\n",
      " [187.60722 ]\n",
      " [156.56093 ]\n",
      " [147.1799  ]\n",
      " [190.83228 ]\n",
      " [151.98906 ]\n",
      " [172.09541 ]\n",
      " [177.42056 ]\n",
      " [157.70116 ]\n",
      " [171.59918 ]\n",
      " [176.58481 ]\n",
      " [162.81375 ]\n",
      " [151.06425 ]\n",
      " [194.37708 ]]\n",
      "\n",
      "1200\n",
      "25.317804\n",
      "[[149.51553]\n",
      " [188.81525]\n",
      " [181.38287]\n",
      " [197.58194]\n",
      " [146.14627]\n",
      " [108.70555]\n",
      " [145.26915]\n",
      " [103.79048]\n",
      " [178.53644]\n",
      " [166.15834]\n",
      " [142.63112]\n",
      " [144.61823]\n",
      " [187.56793]\n",
      " [156.4649 ]\n",
      " [147.28813]\n",
      " [190.77783]\n",
      " [151.79195]\n",
      " [172.32625]\n",
      " [177.40927]\n",
      " [157.71957]\n",
      " [171.71515]\n",
      " [176.53339]\n",
      " [162.93188]\n",
      " [151.05951]\n",
      " [194.2844 ]]\n",
      "\n",
      "1300\n",
      "24.37753\n",
      "[[149.59735 ]\n",
      " [188.71303 ]\n",
      " [181.38277 ]\n",
      " [197.61533 ]\n",
      " [145.99083 ]\n",
      " [108.619255]\n",
      " [145.40031 ]\n",
      " [104.04206 ]\n",
      " [178.43535 ]\n",
      " [166.12088 ]\n",
      " [142.66325 ]\n",
      " [144.57428 ]\n",
      " [187.52972 ]\n",
      " [156.3714  ]\n",
      " [147.39365 ]\n",
      " [190.72467 ]\n",
      " [151.59984 ]\n",
      " [172.55138 ]\n",
      " [177.39838 ]\n",
      " [157.73764 ]\n",
      " [171.82823 ]\n",
      " [176.48317 ]\n",
      " [163.04712 ]\n",
      " [151.05505 ]\n",
      " [194.19402 ]]\n",
      "\n",
      "1400\n",
      "23.483112\n",
      "[[149.67726]\n",
      " [188.61331]\n",
      " [181.3827 ]\n",
      " [197.64787]\n",
      " [145.83919]\n",
      " [108.53498]\n",
      " [145.52821]\n",
      " [104.28739]\n",
      " [178.33664]\n",
      " [166.08412]\n",
      " [142.69458]\n",
      " [144.53131]\n",
      " [187.49252]\n",
      " [156.2803 ]\n",
      " [147.4965 ]\n",
      " [190.67274]\n",
      " [151.41258]\n",
      " [172.77092]\n",
      " [177.38786]\n",
      " [157.75536]\n",
      " [171.93849]\n",
      " [176.43413]\n",
      " [163.1595 ]\n",
      " [151.05092]\n",
      " [194.10585]]\n",
      "\n",
      "1500\n",
      "22.632353\n",
      "[[149.75526 ]\n",
      " [188.516   ]\n",
      " [181.38269 ]\n",
      " [197.67964 ]\n",
      " [145.69127 ]\n",
      " [108.452675]\n",
      " [145.65292 ]\n",
      " [104.52663 ]\n",
      " [178.24025 ]\n",
      " [166.04808 ]\n",
      " [142.72511 ]\n",
      " [144.4893  ]\n",
      " [187.45633 ]\n",
      " [156.19157 ]\n",
      " [147.59679 ]\n",
      " [190.62206 ]\n",
      " [151.23007 ]\n",
      " [172.98503 ]\n",
      " [177.3777  ]\n",
      " [157.77275 ]\n",
      " [172.04597 ]\n",
      " [176.38622 ]\n",
      " [163.26909 ]\n",
      " [151.04704 ]\n",
      " [194.01988 ]]\n",
      "\n",
      "1600\n",
      "21.823086\n",
      "[[149.83144]\n",
      " [188.42108]\n",
      " [181.38272]\n",
      " [197.71063]\n",
      " [145.54697]\n",
      " [108.3723 ]\n",
      " [145.77455]\n",
      " [104.7599 ]\n",
      " [178.14613]\n",
      " [166.0127 ]\n",
      " [142.75487]\n",
      " [144.44826]\n",
      " [187.4211 ]\n",
      " [156.10516]\n",
      " [147.69455]\n",
      " [190.57254]\n",
      " [151.05219]\n",
      " [173.19385]\n",
      " [177.3679 ]\n",
      " [157.78983]\n",
      " [172.1508 ]\n",
      " [176.33945]\n",
      " [163.37598]\n",
      " [151.04346]\n",
      " [193.936  ]]\n",
      "\n",
      "1700\n",
      "21.053284\n",
      "[[149.9058 ]\n",
      " [188.32848]\n",
      " [181.38277]\n",
      " [197.74083]\n",
      " [145.4062 ]\n",
      " [108.29382]\n",
      " [145.89316]\n",
      " [104.98739]\n",
      " [178.05423]\n",
      " [165.97801]\n",
      " [142.78387]\n",
      " [144.40813]\n",
      " [187.38683]\n",
      " [156.02098]\n",
      " [147.78986]\n",
      " [190.5242 ]\n",
      " [150.8788 ]\n",
      " [173.39749]\n",
      " [177.35844]\n",
      " [157.80656]\n",
      " [172.25302]\n",
      " [176.29376]\n",
      " [163.48024]\n",
      " [151.04012]\n",
      " [193.85423]]\n",
      "\n",
      "1800\n",
      "20.32104\n",
      "[[149.97841]\n",
      " [188.23808]\n",
      " [181.38286]\n",
      " [197.77031]\n",
      " [145.26888]\n",
      " [108.21719]\n",
      " [146.0088 ]\n",
      " [105.20923]\n",
      " [177.9645 ]\n",
      " [165.94398]\n",
      " [142.81215]\n",
      " [144.3689 ]\n",
      " [187.35345]\n",
      " [155.93898]\n",
      " [147.88275]\n",
      " [190.47699]\n",
      " [150.70976]\n",
      " [173.59608]\n",
      " [177.3493 ]\n",
      " [157.82297]\n",
      " [172.35268]\n",
      " [176.24913]\n",
      " [163.5819 ]\n",
      " [151.037  ]\n",
      " [193.77449]]\n",
      "\n",
      "1900\n",
      "19.624546\n",
      "[[150.0493  ]\n",
      " [188.14993 ]\n",
      " [181.383   ]\n",
      " [197.79907 ]\n",
      " [145.13493 ]\n",
      " [108.14237 ]\n",
      " [146.1216  ]\n",
      " [105.425545]\n",
      " [177.87688 ]\n",
      " [165.91064 ]\n",
      " [142.8397  ]\n",
      " [144.33057 ]\n",
      " [187.321   ]\n",
      " [155.85912 ]\n",
      " [147.97334 ]\n",
      " [190.43091 ]\n",
      " [150.54504 ]\n",
      " [173.78976 ]\n",
      " [177.34048 ]\n",
      " [157.83907 ]\n",
      " [172.44986 ]\n",
      " [176.20558 ]\n",
      " [163.68106 ]\n",
      " [151.03412 ]\n",
      " [193.69672 ]]\n",
      "\n",
      "2000\n",
      "18.961979\n",
      "[[150.11852 ]\n",
      " [188.06393 ]\n",
      " [181.38315 ]\n",
      " [197.82712 ]\n",
      " [145.00427 ]\n",
      " [108.069336]\n",
      " [146.23158 ]\n",
      " [105.636505]\n",
      " [177.79134 ]\n",
      " [165.87793 ]\n",
      " [142.86656 ]\n",
      " [144.29309 ]\n",
      " [187.2894  ]\n",
      " [155.78128 ]\n",
      " [148.06165 ]\n",
      " [190.38591 ]\n",
      " [150.38445 ]\n",
      " [173.97867 ]\n",
      " [177.33197 ]\n",
      " [157.85483 ]\n",
      " [172.54462 ]\n",
      " [176.16302 ]\n",
      " [163.77777 ]\n",
      " [151.03146 ]\n",
      " [193.62085 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict = {X: x_data, Y: y_data})\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost_val, hy_val, sep = '\\n', end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200.50162]]\n",
      "[[159.55072]\n",
      " [176.07777]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hypothesis, feed_dict = {X: [[100, 70, 101]]}))  # test할때 X는 2차원으로 들어가야함.\n",
    "print(sess.run(hypothesis, feed_dict = {X: [[60, 70, 110], [90, 100, 80]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치나누기\n",
    "\n",
    "이해가 안된다\n",
    "\n",
    "파일 자체가 너무 커서 한번에 읽기 곤란한 경우 혹은 파일 여러개에서 한번에 읽는 경우\n",
    "\n",
    "1. 파일 합치기\n",
    "\n",
    "```python\n",
    "filename_queue = tf.train.string_input_producer(['data01', 'data02', ...], shuffle = False, name = 'filename_queue')\n",
    "```\n",
    "    \n",
    "2. 읽기\n",
    "\n",
    "```python\n",
    "reder = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "```\n",
    "    \n",
    "3. 값 parsing, 데이터 타입 정의 등..\n",
    "\n",
    "```python\n",
    "record_defaults = [[0.], [0.], [0.], [0.]]\n",
    "xy = tf.decode_csv(value, record_defaults = record_defaults)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-8026cfaf05ae>:4: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-34-8026cfaf05ae>:6: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-34-8026cfaf05ae>:15: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-34-8026cfaf05ae>:41: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "0 Cost: 53842.26 \n",
      "Prediction:\n",
      " [[-67.96625 ]\n",
      " [-84.04663 ]\n",
      " [-81.30504 ]\n",
      " [-90.26649 ]\n",
      " [-63.602077]\n",
      " [-50.484077]\n",
      " [-69.976524]\n",
      " [-55.01928 ]\n",
      " [-81.54185 ]\n",
      " [-79.99163 ]] \n",
      "\n",
      "\n",
      "100 Cost: 26.060421 \n",
      "Prediction:\n",
      " [[155.872   ]\n",
      " [185.27347 ]\n",
      " [183.90135 ]\n",
      " [198.6986  ]\n",
      " [141.75822 ]\n",
      " [101.161934]\n",
      " [146.44452 ]\n",
      " [105.58217 ]\n",
      " [170.38881 ]\n",
      " [153.68204 ]] \n",
      "\n",
      "\n",
      "200 Cost: 24.108164 \n",
      "Prediction:\n",
      " [[155.71898 ]\n",
      " [185.32004 ]\n",
      " [183.81851 ]\n",
      " [198.72943 ]\n",
      " [141.75114 ]\n",
      " [101.370514]\n",
      " [146.58302 ]\n",
      " [105.83937 ]\n",
      " [170.63376 ]\n",
      " [154.19717 ]] \n",
      "\n",
      "\n",
      "300 Cost: 22.331305 \n",
      "Prediction:\n",
      " [[155.57372 ]\n",
      " [185.36331 ]\n",
      " [183.73936 ]\n",
      " [198.75923 ]\n",
      " [141.74258 ]\n",
      " [101.56871 ]\n",
      " [146.71678 ]\n",
      " [106.087906]\n",
      " [170.86653 ]\n",
      " [154.68874 ]] \n",
      "\n",
      "\n",
      "400 Cost: 20.714384 \n",
      "Prediction:\n",
      " [[155.43588]\n",
      " [185.40344]\n",
      " [183.66373]\n",
      " [198.78804]\n",
      " [141.7326 ]\n",
      " [101.757  ]\n",
      " [146.84598]\n",
      " [106.32806]\n",
      " [171.08762]\n",
      " [155.15771]] \n",
      "\n",
      "\n",
      "500 Cost: 19.242758 \n",
      "Prediction:\n",
      " [[155.30508 ]\n",
      " [185.44061 ]\n",
      " [183.59143 ]\n",
      " [198.81586 ]\n",
      " [141.72137 ]\n",
      " [101.935875]\n",
      " [146.97075 ]\n",
      " [106.56016 ]\n",
      " [171.29765 ]\n",
      " [155.6052  ]] \n",
      "\n",
      "\n",
      "600 Cost: 17.903488 \n",
      "Prediction:\n",
      " [[155.18102 ]\n",
      " [185.47498 ]\n",
      " [183.52235 ]\n",
      " [198.84274 ]\n",
      " [141.70894 ]\n",
      " [102.10574 ]\n",
      " [147.09131 ]\n",
      " [106.784485]\n",
      " [171.49712 ]\n",
      " [156.03214 ]] \n",
      "\n",
      "\n",
      "700 Cost: 16.684635 \n",
      "Prediction:\n",
      " [[155.06334 ]\n",
      " [185.5067  ]\n",
      " [183.45634 ]\n",
      " [198.86873 ]\n",
      " [141.6954  ]\n",
      " [102.267075]\n",
      " [147.20778 ]\n",
      " [107.00133 ]\n",
      " [171.68655 ]\n",
      " [156.43947 ]] \n",
      "\n",
      "\n",
      "800 Cost: 15.575445 \n",
      "Prediction:\n",
      " [[154.95178]\n",
      " [185.53592]\n",
      " [183.39328]\n",
      " [198.89386]\n",
      " [141.68088]\n",
      " [102.42024]\n",
      " [147.32031]\n",
      " [107.21097]\n",
      " [171.8664 ]\n",
      " [156.82806]] \n",
      "\n",
      "\n",
      "900 Cost: 14.565924 \n",
      "Prediction:\n",
      " [[154.84596]\n",
      " [185.56276]\n",
      " [183.33296]\n",
      " [198.91809]\n",
      " [141.66542]\n",
      " [102.56564]\n",
      " [147.42905]\n",
      " [107.41365]\n",
      " [172.03711]\n",
      " [157.19878]] \n",
      "\n",
      "\n",
      "1000 Cost: 13.647238 \n",
      "Prediction:\n",
      " [[154.74571]\n",
      " [185.58739]\n",
      " [183.27538]\n",
      " [198.94159]\n",
      " [141.64914]\n",
      " [102.70365]\n",
      " [147.53416]\n",
      " [107.60963]\n",
      " [172.19914]\n",
      " [157.55246]] \n",
      "\n",
      "\n",
      "1100 Cost: 12.811102 \n",
      "Prediction:\n",
      " [[154.6507 ]\n",
      " [185.60991]\n",
      " [183.2203 ]\n",
      " [198.96428]\n",
      " [141.63208]\n",
      " [102.83461]\n",
      " [147.63574]\n",
      " [107.79916]\n",
      " [172.35292]\n",
      " [157.88985]] \n",
      "\n",
      "\n",
      "1200 Cost: 12.050224 \n",
      "Prediction:\n",
      " [[154.56067]\n",
      " [185.63042]\n",
      " [183.1677 ]\n",
      " [198.98622]\n",
      " [141.61435]\n",
      " [102.95887]\n",
      " [147.73393]\n",
      " [107.98244]\n",
      " [172.4988 ]\n",
      " [158.21165]] \n",
      "\n",
      "\n",
      "1300 Cost: 11.357691 \n",
      "Prediction:\n",
      " [[154.4754 ]\n",
      " [185.64906]\n",
      " [183.1174 ]\n",
      " [199.00745]\n",
      " [141.59598]\n",
      " [103.07675]\n",
      " [147.82886]\n",
      " [108.15974]\n",
      " [172.63718]\n",
      " [158.51865]] \n",
      "\n",
      "\n",
      "1400 Cost: 10.727437 \n",
      "Prediction:\n",
      " [[154.39468 ]\n",
      " [185.66597 ]\n",
      " [183.06937 ]\n",
      " [199.02798 ]\n",
      " [141.57706 ]\n",
      " [103.18855 ]\n",
      " [147.92065 ]\n",
      " [108.331245]\n",
      " [172.76842 ]\n",
      " [158.81148 ]] \n",
      "\n",
      "\n",
      "1500 Cost: 10.153822 \n",
      "Prediction:\n",
      " [[154.31822 ]\n",
      " [185.68115 ]\n",
      " [183.02342 ]\n",
      " [199.04782 ]\n",
      " [141.5576  ]\n",
      " [103.294556]\n",
      " [148.00938 ]\n",
      " [108.497154]\n",
      " [172.89284 ]\n",
      " [159.09076 ]] \n",
      "\n",
      "\n",
      "1600 Cost: 9.631744 \n",
      "Prediction:\n",
      " [[154.24588 ]\n",
      " [185.6948  ]\n",
      " [182.97952 ]\n",
      " [199.06702 ]\n",
      " [141.53772 ]\n",
      " [103.39505 ]\n",
      " [148.0952  ]\n",
      " [108.657684]\n",
      " [173.0108  ]\n",
      " [159.35715 ]] \n",
      "\n",
      "\n",
      "1700 Cost: 9.156606 \n",
      "Prediction:\n",
      " [[154.17746]\n",
      " [185.70697]\n",
      " [182.93759]\n",
      " [199.0856 ]\n",
      " [141.51746]\n",
      " [103.4903 ]\n",
      " [148.1782 ]\n",
      " [108.81302]\n",
      " [173.1226 ]\n",
      " [159.61122]] \n",
      "\n",
      "\n",
      "1800 Cost: 8.724161 \n",
      "Prediction:\n",
      " [[154.11272 ]\n",
      " [185.71776 ]\n",
      " [182.8975  ]\n",
      " [199.10358 ]\n",
      " [141.49683 ]\n",
      " [103.58056 ]\n",
      " [148.25848 ]\n",
      " [108.963326]\n",
      " [173.22855 ]\n",
      " [159.85352 ]] \n",
      "\n",
      "\n",
      "1900 Cost: 8.330504 \n",
      "Prediction:\n",
      " [[154.05148]\n",
      " [185.7272 ]\n",
      " [182.85918]\n",
      " [199.12096]\n",
      " [141.47588]\n",
      " [103.66608]\n",
      " [148.33614]\n",
      " [109.10882]\n",
      " [173.3289 ]\n",
      " [160.0846 ]] \n",
      "\n",
      "\n",
      "2000 Cost: 7.972191 \n",
      "Prediction:\n",
      " [[153.99362]\n",
      " [185.73546]\n",
      " [182.82257]\n",
      " [199.13779]\n",
      " [141.45471]\n",
      " [103.74706]\n",
      " [148.41129]\n",
      " [109.24964]\n",
      " [173.42397]\n",
      " [160.305  ]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    ['/home/data/data-01-test-score.csv'], shuffle=False, name='filename_queue')\n",
    "\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# Default values, in case of empty columns. Also specifies the type of the\n",
    "# decoded result.\n",
    "record_defaults = [[0.], [0.], [0.], [0.]]  # 각각 field의 데이터 타입\n",
    "xy = tf.decode_csv(value, record_defaults=record_defaults)  # csv decode\n",
    "\n",
    "# collect batches of csv in\n",
    "train_x_batch, train_y_batch = tf.train.batch([xy[0:-1], xy[-1:]], batch_size=10)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Start populating the filename queue.\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "for step in range(2001):\n",
    "    x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_batch, Y: y_batch})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost:\", cost_val, \"\\nPrediction:\\n\", hy_val, '\\n\\n')\n",
    "\n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method BaseSession.close of <tensorflow.python.client.session.Session object at 0x7fc7557a1470>>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast, tf.equal 작동\n",
    "sess = tf.Session()\n",
    "print(sess.run(tf.cast(1 > 0.5, dtype = tf.float32)))\n",
    "print(sess.run(tf.cast(tf.equal(1, 1), dtype = tf.float32)))\n",
    "sess.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\t\t1.1903245\n",
      " 1000\t\t0.3544367\n",
      " 2000\t\t0.2985876\n",
      " 3000\t\t0.2611195\n",
      " 4000\t\t0.2315915\n",
      " 5000\t\t0.2077589\n",
      " 6000\t\t0.1882218\n",
      " 7000\t\t0.1719754\n",
      " 8000\t\t0.1582860\n",
      " 9000\t\t0.1466126\n",
      "10000\t\t0.1365507\n",
      "\n",
      "\n",
      "\n",
      "[[0.02548951]\n",
      " [0.15105806]\n",
      " [0.27865526]\n",
      " [0.7936255 ]\n",
      " [0.9470268 ]\n",
      " [0.9826974 ]]\n",
      "\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print('{:5d}\\t\\t{:.7f}'.format(step, cost_val))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print('\\n', h, c, a, sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8)\n",
      "\n",
      "[[-0.294118    0.487437    0.180328   ...  0.00149028 -0.53117\n",
      "  -0.0333333 ]\n",
      " [-0.882353   -0.145729    0.0819672  ... -0.207153   -0.766866\n",
      "  -0.666667  ]\n",
      " [-0.0588235   0.839196    0.0491803  ... -0.305514   -0.492741\n",
      "  -0.633333  ]\n",
      " ...\n",
      " [-0.411765    0.21608     0.180328   ... -0.219076   -0.857387\n",
      "  -0.7       ]\n",
      " [-0.882353    0.266332   -0.0163934  ... -0.102832   -0.768574\n",
      "  -0.133333  ]\n",
      " [-0.882353   -0.0653266   0.147541   ... -0.0938897  -0.797609\n",
      "  -0.933333  ]]\n",
      "\n",
      "(759, 1)\n",
      "\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('/home/data/data-03-diabetes.csv', delimiter = ',', dtype = np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "print(x_data.shape, x_data, y_data.shape, y_data, sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "W = tf.Variable(tf.random_normal([8, 1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.005).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6127287\n",
      "1000 0.558461\n",
      "2000 0.54250884\n",
      "3000 0.5307415\n",
      "4000 0.5216248\n",
      "5000 0.5144174\n",
      "6000 0.5086194\n",
      "7000 0.50388443\n",
      "8000 0.49996662\n",
      "9000 0.4966882\n",
      "10000 0.4939178\n",
      "\n",
      "\n",
      "\n",
      "[[0.45798242]\n",
      " [0.89713764]\n",
      " [0.27737203]\n",
      " [0.9266707 ]\n",
      " [0.24643958]\n",
      " [0.7194366 ]\n",
      " [0.9210888 ]\n",
      " [0.58687335]\n",
      " [0.34609097]\n",
      " [0.51116246]\n",
      " [0.66533285]\n",
      " [0.22504002]\n",
      " [0.27487022]\n",
      " [0.38893768]\n",
      " [0.7046885 ]\n",
      " [0.46708536]\n",
      " [0.68802017]\n",
      " [0.86845124]\n",
      " [0.80598336]\n",
      " [0.58484757]\n",
      " [0.6296144 ]\n",
      " [0.15931612]\n",
      " [0.61933166]\n",
      " [0.67018455]\n",
      " [0.41001517]\n",
      " [0.91227055]\n",
      " [0.55598193]\n",
      " [0.6090165 ]\n",
      " [0.6972948 ]\n",
      " [0.481177  ]\n",
      " [0.93319535]\n",
      " [0.8013827 ]\n",
      " [0.5649214 ]\n",
      " [0.79450476]\n",
      " [0.3806644 ]\n",
      " [0.6145935 ]\n",
      " [0.7938241 ]\n",
      " [0.55820566]\n",
      " [0.5425578 ]\n",
      " [0.38879067]\n",
      " [0.78150964]\n",
      " [0.2071487 ]\n",
      " [0.44846582]\n",
      " [0.09872609]\n",
      " [0.61089426]\n",
      " [0.8966268 ]\n",
      " [0.69322956]\n",
      " [0.68638533]\n",
      " [0.9101236 ]\n",
      " [0.9181371 ]\n",
      " [0.90469193]\n",
      " [0.2965937 ]\n",
      " [0.40898377]\n",
      " [0.95045054]\n",
      " [0.29538262]\n",
      " [0.46476546]\n",
      " [0.16592935]\n",
      " [0.6978337 ]\n",
      " [0.8469635 ]\n",
      " [0.51231664]\n",
      " [0.910761  ]\n",
      " [0.71820855]\n",
      " [0.6474412 ]\n",
      " [0.81474805]\n",
      " [0.5594447 ]\n",
      " [0.5748856 ]\n",
      " [0.9376856 ]\n",
      " [0.6885347 ]\n",
      " [0.83529896]\n",
      " [0.67464286]\n",
      " [0.28179818]\n",
      " [0.7051712 ]\n",
      " [0.8834045 ]\n",
      " [0.9092341 ]\n",
      " [0.8235749 ]\n",
      " [0.7578858 ]\n",
      " [0.45755315]\n",
      " [0.8396719 ]\n",
      " [0.8816514 ]\n",
      " [0.8871987 ]\n",
      " [0.82275784]\n",
      " [0.77979827]\n",
      " [0.3879298 ]\n",
      " [0.7921116 ]\n",
      " [0.5126434 ]\n",
      " [0.8552908 ]\n",
      " [0.44777837]\n",
      " [0.8736975 ]\n",
      " [0.91587555]\n",
      " [0.7570647 ]\n",
      " [0.78124595]\n",
      " [0.6612971 ]\n",
      " [0.72385347]\n",
      " [0.60807383]\n",
      " [0.87774014]\n",
      " [0.96268106]\n",
      " [0.86861414]\n",
      " [0.6119094 ]\n",
      " [0.3192144 ]\n",
      " [0.66441953]\n",
      " [0.5734203 ]\n",
      " [0.938993  ]\n",
      " [0.74093133]\n",
      " [0.7758912 ]\n",
      " [0.81326663]\n",
      " [0.7129438 ]\n",
      " [0.9010216 ]\n",
      " [0.7772745 ]\n",
      " [0.5534119 ]\n",
      " [0.41558483]\n",
      " [0.90622556]\n",
      " [0.8332751 ]\n",
      " [0.4987716 ]\n",
      " [0.46625912]\n",
      " [0.6259873 ]\n",
      " [0.8030226 ]\n",
      " [0.8365203 ]\n",
      " [0.90029436]\n",
      " [0.20406502]\n",
      " [0.70444816]\n",
      " [0.8292148 ]\n",
      " [0.6298609 ]\n",
      " [0.5939376 ]\n",
      " [0.8305279 ]\n",
      " [0.71177965]\n",
      " [0.8323171 ]\n",
      " [0.7962559 ]\n",
      " [0.58146393]\n",
      " [0.57142806]\n",
      " [0.42975888]\n",
      " [0.50593394]\n",
      " [0.73543525]\n",
      " [0.9096935 ]\n",
      " [0.8322679 ]\n",
      " [0.76563454]\n",
      " [0.8158082 ]\n",
      " [0.4441645 ]\n",
      " [0.7745398 ]\n",
      " [0.71723616]\n",
      " [0.7053367 ]\n",
      " [0.86998284]\n",
      " [0.61104304]\n",
      " [0.6074248 ]\n",
      " [0.6768559 ]\n",
      " [0.8572364 ]\n",
      " [0.73156226]\n",
      " [0.50114226]\n",
      " [0.9040426 ]\n",
      " [0.64793587]\n",
      " [0.74964803]\n",
      " [0.30736852]\n",
      " [0.43482164]\n",
      " [0.18040556]\n",
      " [0.2978915 ]\n",
      " [0.9011371 ]\n",
      " [0.85769695]\n",
      " [0.9181324 ]\n",
      " [0.15783975]\n",
      " [0.5313359 ]\n",
      " [0.7535554 ]\n",
      " [0.5998532 ]\n",
      " [0.8504988 ]\n",
      " [0.40920514]\n",
      " [0.77688885]\n",
      " [0.6393708 ]\n",
      " [0.62984496]\n",
      " [0.7022149 ]\n",
      " [0.8327806 ]\n",
      " [0.7208028 ]\n",
      " [0.6373838 ]\n",
      " [0.8558897 ]\n",
      " [0.85878277]\n",
      " [0.9303057 ]\n",
      " [0.29316625]\n",
      " [0.7670274 ]\n",
      " [0.28506505]\n",
      " [0.44557655]\n",
      " [0.41830066]\n",
      " [0.82779837]\n",
      " [0.6627439 ]\n",
      " [0.9065932 ]\n",
      " [0.8683331 ]\n",
      " [0.6025277 ]\n",
      " [0.21781105]\n",
      " [0.26874018]\n",
      " [0.5891521 ]\n",
      " [0.6961612 ]\n",
      " [0.63095284]\n",
      " [0.8114928 ]\n",
      " [0.6155885 ]\n",
      " [0.40839803]\n",
      " [0.27504045]\n",
      " [0.87047625]\n",
      " [0.4333936 ]\n",
      " [0.8492968 ]\n",
      " [0.8757434 ]\n",
      " [0.70517635]\n",
      " [0.6711856 ]\n",
      " [0.6028607 ]\n",
      " [0.5861993 ]\n",
      " [0.6754807 ]\n",
      " [0.92433655]\n",
      " [0.7560969 ]\n",
      " [0.7745912 ]\n",
      " [0.21258667]\n",
      " [0.35776585]\n",
      " [0.89359105]\n",
      " [0.27527612]\n",
      " [0.9086782 ]\n",
      " [0.3074921 ]\n",
      " [0.32071367]\n",
      " [0.51669997]\n",
      " [0.6757724 ]\n",
      " [0.26299006]\n",
      " [0.739306  ]\n",
      " [0.71197677]\n",
      " [0.7571599 ]\n",
      " [0.66697705]\n",
      " [0.22326624]\n",
      " [0.4060715 ]\n",
      " [0.68841356]\n",
      " [0.57196134]\n",
      " [0.9019288 ]\n",
      " [0.91297853]\n",
      " [0.6507912 ]\n",
      " [0.4531837 ]\n",
      " [0.06824914]\n",
      " [0.6485075 ]\n",
      " [0.38952315]\n",
      " [0.49911222]\n",
      " [0.92127544]\n",
      " [0.6340525 ]\n",
      " [0.924095  ]\n",
      " [0.3025164 ]\n",
      " [0.18926892]\n",
      " [0.31491125]\n",
      " [0.6887144 ]\n",
      " [0.89565015]\n",
      " [0.8498549 ]\n",
      " [0.65816844]\n",
      " [0.6710933 ]\n",
      " [0.62342453]\n",
      " [0.2013205 ]\n",
      " [0.54528713]\n",
      " [0.17935655]\n",
      " [0.5749281 ]\n",
      " [0.8350414 ]\n",
      " [0.6586308 ]\n",
      " [0.6784714 ]\n",
      " [0.92466646]\n",
      " [0.7664374 ]\n",
      " [0.7294642 ]\n",
      " [0.76412773]\n",
      " [0.75584793]\n",
      " [0.83740985]\n",
      " [0.49857453]\n",
      " [0.5110332 ]\n",
      " [0.54589945]\n",
      " [0.7788563 ]\n",
      " [0.6813496 ]\n",
      " [0.6850822 ]\n",
      " [0.78547895]\n",
      " [0.3594969 ]\n",
      " [0.52552634]\n",
      " [0.59092426]\n",
      " [0.64669216]\n",
      " [0.4069128 ]\n",
      " [0.8909831 ]\n",
      " [0.73357993]\n",
      " [0.9019443 ]\n",
      " [0.5350763 ]\n",
      " [0.7579452 ]\n",
      " [0.784622  ]\n",
      " [0.7867247 ]\n",
      " [0.6528163 ]\n",
      " [0.8383    ]\n",
      " [0.38930818]\n",
      " [0.58808154]\n",
      " [0.6699768 ]\n",
      " [0.3976105 ]\n",
      " [0.7813357 ]\n",
      " [0.33664787]\n",
      " [0.6183486 ]\n",
      " [0.915473  ]\n",
      " [0.7643139 ]\n",
      " [0.80599976]\n",
      " [0.678122  ]\n",
      " [0.52366674]\n",
      " [0.6927384 ]\n",
      " [0.47564745]\n",
      " [0.5166462 ]\n",
      " [0.6575611 ]\n",
      " [0.59617543]\n",
      " [0.6102354 ]\n",
      " [0.59259707]\n",
      " [0.26576835]\n",
      " [0.7087244 ]\n",
      " [0.8697999 ]\n",
      " [0.49595165]\n",
      " [0.6271877 ]\n",
      " [0.7596707 ]\n",
      " [0.5138856 ]\n",
      " [0.72191113]\n",
      " [0.5071524 ]\n",
      " [0.70846134]\n",
      " [0.8607672 ]\n",
      " [0.6401613 ]\n",
      " [0.7110559 ]\n",
      " [0.85379755]\n",
      " [0.5362396 ]\n",
      " [0.8415558 ]\n",
      " [0.91468453]\n",
      " [0.37317938]\n",
      " [0.78137106]\n",
      " [0.32330233]\n",
      " [0.7721752 ]\n",
      " [0.78346956]\n",
      " [0.6772171 ]\n",
      " [0.36508453]\n",
      " [0.7800152 ]\n",
      " [0.74966997]\n",
      " [0.75298446]\n",
      " [0.2452082 ]\n",
      " [0.7941464 ]\n",
      " [0.8252857 ]\n",
      " [0.5487738 ]\n",
      " [0.92658293]\n",
      " [0.37476593]\n",
      " [0.63281596]\n",
      " [0.93113005]\n",
      " [0.28745353]\n",
      " [0.4862724 ]\n",
      " [0.66562414]\n",
      " [0.36650828]\n",
      " [0.23994383]\n",
      " [0.83125985]\n",
      " [0.8913458 ]\n",
      " [0.829803  ]\n",
      " [0.5979904 ]\n",
      " [0.64584064]\n",
      " [0.57720685]\n",
      " [0.7901442 ]\n",
      " [0.78617644]\n",
      " [0.91163176]\n",
      " [0.7145578 ]\n",
      " [0.7307914 ]\n",
      " [0.5780335 ]\n",
      " [0.902658  ]\n",
      " [0.9217758 ]\n",
      " [0.7143512 ]\n",
      " [0.30972597]\n",
      " [0.7140793 ]\n",
      " [0.39473814]\n",
      " [0.7152518 ]\n",
      " [0.297718  ]\n",
      " [0.33723158]\n",
      " [0.4877896 ]\n",
      " [0.6567922 ]\n",
      " [0.44378942]\n",
      " [0.5979962 ]\n",
      " [0.82935315]\n",
      " [0.626574  ]\n",
      " [0.81793326]\n",
      " [0.9164313 ]\n",
      " [0.7380016 ]\n",
      " [0.13327312]\n",
      " [0.49619514]\n",
      " [0.81717443]\n",
      " [0.8473582 ]\n",
      " [0.70199764]\n",
      " [0.3210032 ]\n",
      " [0.8327042 ]\n",
      " [0.8776125 ]\n",
      " [0.3758306 ]\n",
      " [0.56620985]\n",
      " [0.81764424]\n",
      " [0.79632175]\n",
      " [0.85809815]\n",
      " [0.8770504 ]\n",
      " [0.8558886 ]\n",
      " [0.9080739 ]\n",
      " [0.6643779 ]\n",
      " [0.57919466]\n",
      " [0.5838763 ]\n",
      " [0.81234896]\n",
      " [0.8619078 ]\n",
      " [0.30589867]\n",
      " [0.80976033]\n",
      " [0.8503898 ]\n",
      " [0.37349725]\n",
      " [0.6308908 ]\n",
      " [0.8429818 ]\n",
      " [0.5538924 ]\n",
      " [0.8687136 ]\n",
      " [0.36544532]\n",
      " [0.802927  ]\n",
      " [0.6260489 ]\n",
      " [0.8237866 ]\n",
      " [0.43592504]\n",
      " [0.7365416 ]\n",
      " [0.7023186 ]\n",
      " [0.7376344 ]\n",
      " [0.14603189]\n",
      " [0.31128556]\n",
      " [0.63018215]\n",
      " [0.80321723]\n",
      " [0.5018025 ]\n",
      " [0.7953671 ]\n",
      " [0.5413373 ]\n",
      " [0.41641617]\n",
      " [0.81171215]\n",
      " [0.49398506]\n",
      " [0.88040435]\n",
      " [0.8063005 ]\n",
      " [0.6691154 ]\n",
      " [0.8989576 ]\n",
      " [0.66873413]\n",
      " [0.7960785 ]\n",
      " [0.3934723 ]\n",
      " [0.3639289 ]\n",
      " [0.7127983 ]\n",
      " [0.5343209 ]\n",
      " [0.46309847]\n",
      " [0.8741262 ]\n",
      " [0.8644439 ]\n",
      " [0.88559014]\n",
      " [0.92695284]\n",
      " [0.6796776 ]\n",
      " [0.860489  ]\n",
      " [0.42173168]\n",
      " [0.37962618]\n",
      " [0.49663076]\n",
      " [0.91948813]\n",
      " [0.56081444]\n",
      " [0.2152845 ]\n",
      " [0.9056343 ]\n",
      " [0.80258775]\n",
      " [0.54864013]\n",
      " [0.79008263]\n",
      " [0.04334843]\n",
      " [0.89599216]\n",
      " [0.7233117 ]\n",
      " [0.7259516 ]\n",
      " [0.73787165]\n",
      " [0.9453349 ]\n",
      " [0.6324781 ]\n",
      " [0.7421533 ]\n",
      " [0.7317982 ]\n",
      " [0.8476622 ]\n",
      " [0.2525829 ]\n",
      " [0.66930383]\n",
      " [0.8770982 ]\n",
      " [0.627647  ]\n",
      " [0.71759343]\n",
      " [0.9119615 ]\n",
      " [0.8038881 ]\n",
      " [0.8391006 ]\n",
      " [0.4458166 ]\n",
      " [0.77701735]\n",
      " [0.916639  ]\n",
      " [0.72088176]\n",
      " [0.6353086 ]\n",
      " [0.38130683]\n",
      " [0.5114873 ]\n",
      " [0.53293175]\n",
      " [0.61267006]\n",
      " [0.517887  ]\n",
      " [0.75255466]\n",
      " [0.57205594]\n",
      " [0.7495192 ]\n",
      " [0.7908118 ]\n",
      " [0.71440995]\n",
      " [0.6484503 ]\n",
      " [0.55139405]\n",
      " [0.557085  ]\n",
      " [0.9138628 ]\n",
      " [0.7924944 ]\n",
      " [0.32162756]\n",
      " [0.4749967 ]\n",
      " [0.576763  ]\n",
      " [0.1755417 ]\n",
      " [0.8519423 ]\n",
      " [0.21237105]\n",
      " [0.87752813]\n",
      " [0.836256  ]\n",
      " [0.80497456]\n",
      " [0.6876706 ]\n",
      " [0.8568288 ]\n",
      " [0.42069   ]\n",
      " [0.73912233]\n",
      " [0.9136668 ]\n",
      " [0.39894664]\n",
      " [0.47771758]\n",
      " [0.84715694]\n",
      " [0.8408747 ]\n",
      " [0.64540654]\n",
      " [0.7886622 ]\n",
      " [0.78488404]\n",
      " [0.7338277 ]\n",
      " [0.3351484 ]\n",
      " [0.779232  ]\n",
      " [0.88772094]\n",
      " [0.60229665]\n",
      " [0.73412776]\n",
      " [0.7105711 ]\n",
      " [0.765435  ]\n",
      " [0.8292949 ]\n",
      " [0.9137473 ]\n",
      " [0.64877045]\n",
      " [0.4327245 ]\n",
      " [0.72048277]\n",
      " [0.7246421 ]\n",
      " [0.94525284]\n",
      " [0.7486911 ]\n",
      " [0.68277365]\n",
      " [0.4335033 ]\n",
      " [0.71570635]\n",
      " [0.89925003]\n",
      " [0.93316543]\n",
      " [0.8685328 ]\n",
      " [0.6759358 ]\n",
      " [0.6214695 ]\n",
      " [0.80258363]\n",
      " [0.46998772]\n",
      " [0.79750466]\n",
      " [0.75976443]\n",
      " [0.8719563 ]\n",
      " [0.6256319 ]\n",
      " [0.64751637]\n",
      " [0.8483433 ]\n",
      " [0.4868027 ]\n",
      " [0.5310575 ]\n",
      " [0.64251167]\n",
      " [0.7243342 ]\n",
      " [0.6111903 ]\n",
      " [0.8678726 ]\n",
      " [0.89643794]\n",
      " [0.26592454]\n",
      " [0.22917998]\n",
      " [0.7606372 ]\n",
      " [0.5829913 ]\n",
      " [0.27189946]\n",
      " [0.8270066 ]\n",
      " [0.87399715]\n",
      " [0.665594  ]\n",
      " [0.916229  ]\n",
      " [0.89790654]\n",
      " [0.73735964]\n",
      " [0.8180057 ]\n",
      " [0.66403043]\n",
      " [0.56605405]\n",
      " [0.69704217]\n",
      " [0.61251473]\n",
      " [0.19860238]\n",
      " [0.8815095 ]\n",
      " [0.85610473]\n",
      " [0.65090597]\n",
      " [0.8980978 ]\n",
      " [0.85659003]\n",
      " [0.8617915 ]\n",
      " [0.6229507 ]\n",
      " [0.68615276]\n",
      " [0.84916526]\n",
      " [0.69041824]\n",
      " [0.84034634]\n",
      " [0.8938872 ]\n",
      " [0.5916512 ]\n",
      " [0.81823635]\n",
      " [0.8173439 ]\n",
      " [0.57452327]\n",
      " [0.51921403]\n",
      " [0.1681692 ]\n",
      " [0.30724454]\n",
      " [0.80102444]\n",
      " [0.6155528 ]\n",
      " [0.6482004 ]\n",
      " [0.5493021 ]\n",
      " [0.91410995]\n",
      " [0.48028424]\n",
      " [0.76667976]\n",
      " [0.35381132]\n",
      " [0.8400379 ]\n",
      " [0.35525113]\n",
      " [0.7597493 ]\n",
      " [0.58714956]\n",
      " [0.83497536]\n",
      " [0.5851167 ]\n",
      " [0.29778033]\n",
      " [0.7455224 ]\n",
      " [0.93749404]\n",
      " [0.46504533]\n",
      " [0.91207844]\n",
      " [0.8292203 ]\n",
      " [0.80775326]\n",
      " [0.7898449 ]\n",
      " [0.47483036]\n",
      " [0.4054113 ]\n",
      " [0.72396827]\n",
      " [0.26261994]\n",
      " [0.9280292 ]\n",
      " [0.3858706 ]\n",
      " [0.9069871 ]\n",
      " [0.8667377 ]\n",
      " [0.51983905]\n",
      " [0.25902653]\n",
      " [0.6529721 ]\n",
      " [0.4730203 ]\n",
      " [0.7943444 ]\n",
      " [0.6450077 ]\n",
      " [0.96382725]\n",
      " [0.5192662 ]\n",
      " [0.6195227 ]\n",
      " [0.7595544 ]\n",
      " [0.7492688 ]\n",
      " [0.11442021]\n",
      " [0.7399785 ]\n",
      " [0.79504704]\n",
      " [0.80044496]\n",
      " [0.6189585 ]\n",
      " [0.4768285 ]\n",
      " [0.60326886]\n",
      " [0.87835574]\n",
      " [0.6110975 ]\n",
      " [0.7332355 ]\n",
      " [0.79020286]\n",
      " [0.83558255]\n",
      " [0.7472581 ]\n",
      " [0.5265696 ]\n",
      " [0.7575848 ]\n",
      " [0.87094176]\n",
      " [0.6796314 ]\n",
      " [0.9357686 ]\n",
      " [0.7414892 ]\n",
      " [0.61985546]\n",
      " [0.5105463 ]\n",
      " [0.79256094]\n",
      " [0.81566083]\n",
      " [0.54026777]\n",
      " [0.68553543]\n",
      " [0.33666477]\n",
      " [0.5475083 ]\n",
      " [0.7740133 ]\n",
      " [0.93034595]\n",
      " [0.8321713 ]\n",
      " [0.71691096]\n",
      " [0.74751425]\n",
      " [0.86802614]\n",
      " [0.58101344]\n",
      " [0.9081789 ]\n",
      " [0.5902731 ]\n",
      " [0.7998475 ]\n",
      " [0.34503713]\n",
      " [0.15324304]\n",
      " [0.31351402]\n",
      " [0.38861918]\n",
      " [0.6983378 ]\n",
      " [0.8028328 ]\n",
      " [0.6275488 ]\n",
      " [0.70371056]\n",
      " [0.79679847]\n",
      " [0.53058416]\n",
      " [0.45949277]\n",
      " [0.886837  ]\n",
      " [0.82714105]\n",
      " [0.43573558]\n",
      " [0.63983196]\n",
      " [0.25891256]\n",
      " [0.40566316]\n",
      " [0.70735383]\n",
      " [0.7198948 ]\n",
      " [0.87737286]\n",
      " [0.9609575 ]\n",
      " [0.2627598 ]\n",
      " [0.70802945]\n",
      " [0.59236526]\n",
      " [0.48059583]\n",
      " [0.72045267]\n",
      " [0.7117502 ]\n",
      " [0.88173676]\n",
      " [0.71696573]\n",
      " [0.5602652 ]\n",
      " [0.5936454 ]\n",
      " [0.17566255]\n",
      " [0.6899644 ]\n",
      " [0.5642569 ]\n",
      " [0.87629926]\n",
      " [0.5612626 ]\n",
      " [0.6162845 ]\n",
      " [0.7410625 ]\n",
      " [0.70650625]\n",
      " [0.4982258 ]\n",
      " [0.7445279 ]\n",
      " [0.6323519 ]\n",
      " [0.40136474]\n",
      " [0.64701504]\n",
      " [0.8557606 ]\n",
      " [0.79258245]\n",
      " [0.57789135]\n",
      " [0.8019843 ]\n",
      " [0.33919156]\n",
      " [0.82681376]\n",
      " [0.6375962 ]\n",
      " [0.7421786 ]\n",
      " [0.4570782 ]\n",
      " [0.66839856]\n",
      " [0.79802424]\n",
      " [0.26448703]\n",
      " [0.36839736]\n",
      " [0.7573494 ]\n",
      " [0.80718744]\n",
      " [0.7681563 ]\n",
      " [0.8626415 ]\n",
      " [0.7979582 ]\n",
      " [0.7261857 ]\n",
      " [0.7173358 ]\n",
      " [0.7360025 ]\n",
      " [0.68684584]\n",
      " [0.7784542 ]\n",
      " [0.48205802]\n",
      " [0.46301913]\n",
      " [0.86053395]\n",
      " [0.7815174 ]\n",
      " [0.58824533]\n",
      " [0.3671149 ]\n",
      " [0.8623551 ]\n",
      " [0.781644  ]\n",
      " [0.8125365 ]\n",
      " [0.64226174]\n",
      " [0.83554924]\n",
      " [0.86343235]\n",
      " [0.7608446 ]\n",
      " [0.4386633 ]\n",
      " [0.88423413]\n",
      " [0.89688325]\n",
      " [0.33669126]\n",
      " [0.19557735]\n",
      " [0.66932833]\n",
      " [0.4546285 ]\n",
      " [0.784235  ]\n",
      " [0.43309495]\n",
      " [0.50249976]\n",
      " [0.4504637 ]\n",
      " [0.76881355]\n",
      " [0.84303606]\n",
      " [0.19370529]\n",
      " [0.4169833 ]\n",
      " [0.6145714 ]\n",
      " [0.49206805]\n",
      " [0.5314039 ]\n",
      " [0.7649149 ]\n",
      " [0.2297036 ]\n",
      " [0.89418817]\n",
      " [0.26161015]\n",
      " [0.7956387 ]\n",
      " [0.69414335]\n",
      " [0.72833496]\n",
      " [0.8036521 ]\n",
      " [0.7281773 ]\n",
      " [0.8646241 ]]\n",
      "\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "0.7615283\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict = feed)\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = feed))\n",
    "            \n",
    "    print('\\n', *sess.run([hypothesis, predicted, accuracy], feed_dict = feed), sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1, 1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 1e-3).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0453628\n",
      "2000 0.71783376\n",
      "4000 0.65588987\n",
      "6000 0.6319225\n",
      "8000 0.61513215\n",
      "10000 0.60111433\n",
      "12000 0.58899176\n",
      "14000 0.5784235\n",
      "16000 0.5691797\n",
      "18000 0.56106967\n",
      "20000 0.5539306\n",
      "0.7088274\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    for step in range(20001):\n",
    "        sess.run(train, feed_dict = feed)\n",
    "        if step % 2000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = feed))\n",
    "            \n",
    "    print(sess.run(accuracy, feed_dict = feed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 06-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 3])\n",
    "W = tf.Variable(tf.random_normal([4, 3]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([3]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.151659\n",
      "200 0.49153733\n",
      "400 0.38063246\n",
      "600 0.28467178\n",
      "800 0.23825255\n",
      "1000 0.21467552\n",
      "1200 0.19544351\n",
      "1400 0.17938064\n",
      "1600 0.16573608\n",
      "1800 0.15399241\n",
      "2000 0.1437757\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-46-773b5cb1363b>:11: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n",
      "[[1.4556615e-03 9.9853563e-01 8.6868640e-06]] [1]\n",
      "\n",
      "[[1.4556629e-03 9.9853563e-01 8.6868804e-06]\n",
      " [9.2186505e-01 7.0028231e-02 8.1066890e-03]\n",
      " [2.8308402e-06 1.0764663e-03 9.9892068e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    for step in range(2001):\n",
    "        sess.run(train, feed_dict = feed)\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = feed))\n",
    "            \n",
    "    a = sess.run(hypothesis, feed_dict = {X: [[1, 11, 7, 9]]})\n",
    "    print()\n",
    "    print(a, sess.run(tf.arg_max(a, 1)))\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 2, 1, 1]]})\n",
    "    print()\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML lab 06-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16)\n",
      "\n",
      "(101, 1)\n",
      "\n",
      "[[1. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 1. 0. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 1. 0. 0.]]\n",
      "\n",
      "[[0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]\n",
      " [6.]\n",
      " [6.]\n",
      " [6.]\n",
      " [1.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [5.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [3.]\n",
      " [5.]\n",
      " [5.]\n",
      " [1.]\n",
      " [5.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [6.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [5.]\n",
      " [4.]\n",
      " [6.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [6.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [6.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [6.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [6.]\n",
      " [3.]\n",
      " [1.]\n",
      " [5.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [5.]\n",
      " [0.]\n",
      " [6.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('/home/data/data-04-zoo.csv', delimiter = ',', dtype = np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape, x_data, y_data, sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-58c13ac3d4f0>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classes = len(np.unique(y_data))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, x_data.shape[1]])\n",
    "Y = tf.placeholder(tf.int32, shape = [None, y_data.shape[1]])\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1, nb_classes]), name = 'bias')\n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_one_hot)\n",
    "\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.0241985 0.04950495\n",
      "100 0.54039913 0.83168316\n",
      "200 0.33802545 0.9009901\n",
      "300 0.2513465 0.9306931\n",
      "400 0.20061125 0.95049506\n",
      "500 0.16706587 0.95049506\n",
      "600 0.14322996 0.980198\n",
      "700 0.12543094 1.0\n",
      "800 0.11164228 1.0\n",
      "900 0.1006515 1.0\n",
      "1000 0.0916878 1.0\n",
      "1100 0.08423816 1.0\n",
      "1200 0.07794823 1.0\n",
      "1300 0.07256571 1.0\n",
      "1400 0.06790617 1.0\n",
      "1500 0.06383213 1.0\n",
      "1600 0.06023865 1.0\n",
      "1700 0.05704456 1.0\n",
      "1800 0.054185975 1.0\n",
      "1900 0.051612053 1.0\n",
      "2000 0.049281698 1.0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 3 3\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 3 3\n",
      "True 3 3\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 1 1\n",
      "True 3 3\n",
      "True 6 6\n",
      "True 6 6\n",
      "True 6 6\n",
      "True 1 1\n",
      "True 0 0\n",
      "True 3 3\n",
      "True 0 0\n",
      "True 1 1\n",
      "True 1 1\n",
      "True 0 0\n",
      "True 1 1\n",
      "True 5 5\n",
      "True 4 4\n",
      "True 4 4\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 5 5\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 1 1\n",
      "True 3 3\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 1 1\n",
      "True 3 3\n",
      "True 5 5\n",
      "True 5 5\n",
      "True 1 1\n",
      "True 5 5\n",
      "True 1 1\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 6 6\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 5 5\n",
      "True 4 4\n",
      "True 6 6\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 1 1\n",
      "True 1 1\n",
      "True 1 1\n",
      "True 1 1\n",
      "True 3 3\n",
      "True 3 3\n",
      "True 2 2\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 1 1\n",
      "True 6 6\n",
      "True 3 3\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 2 2\n",
      "True 6 6\n",
      "True 1 1\n",
      "True 1 1\n",
      "True 2 2\n",
      "True 6 6\n",
      "True 3 3\n",
      "True 1 1\n",
      "True 0 0\n",
      "True 6 6\n",
      "True 3 3\n",
      "True 1 1\n",
      "True 5 5\n",
      "True 4 4\n",
      "True 2 2\n",
      "True 2 2\n",
      "True 3 3\n",
      "True 0 0\n",
      "True 0 0\n",
      "True 1 1\n",
      "True 0 0\n",
      "True 5 5\n",
      "True 0 0\n",
      "True 6 6\n",
      "True 1 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "            print(step, loss, acc)\n",
    "            \n",
    "    pred = sess.run(prediction, feed_dict = {X: x_data})\n",
    "    \n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(p == int(y), p, int(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('/home/data/data-04-zoo.csv', delimiter = ',', dtype = np.float32)\n",
    "x_data = xy[:, :-1]\n",
    "y_data = xy[:, [-1]]\n",
    "nb_classes = len(np.unique(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, x_data.shape[1]])\n",
    "Y = tf.placeholder(tf.int32, shape = [None, y_data.shape[1]])\n",
    "W = tf.Variable(tf.random_normal([16, 7]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1, 7]), name = 'bias')\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, shape = [-1, nb_classes])\n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost_i = -tf.reduce_sum(Y_one_hot * tf.log(hypothesis), axis = 1)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, axis = 1)\n",
    "correct = tf.argmax(Y_one_hot, axis = 1)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, correct), dtype = tf.float32))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [9.900456, 0.20792079]\n",
      "200 [0.4004227, 0.8910891]\n",
      "400 [0.21384874, 0.96039605]\n",
      "600 [0.15169996, 0.97029704]\n",
      "800 [0.11879678, 0.980198]\n",
      "1000 [0.097711824, 0.990099]\n",
      "1200 [0.08295804, 1.0]\n",
      "1400 [0.07205217, 1.0]\n",
      "1600 [0.06367063, 1.0]\n",
      "1800 [0.05703623, 1.0]\n",
      "2000 [0.05166063, 1.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    for step in range(2001):\n",
    "        sess.run(train, feed_dict = feed)\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run([cost, accuracy], feed_dict = feed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.93331509e+01 1.77208710e+01 7.87592125e+00 1.93331509e+01\n",
      " 1.89318237e+01 1.77208710e+01 1.91275711e+01 9.43546963e+00\n",
      " 7.87592125e+00 1.92329960e+01 1.89318237e+01 2.89304722e-02\n",
      " 7.87592125e+00 2.16927481e+00 1.05025377e+01 1.52142696e+01\n",
      " 6.67761564e-02 1.77208710e+01 7.34143686e+00 7.71816015e+00\n",
      " 2.89304722e-02 9.44509804e-02 1.77208710e+01 2.71933209e-02\n",
      " 1.52787142e+01 1.20033655e+01 1.17772264e+01 8.96824074e+00\n",
      " 1.77208710e+01 1.30812597e+01 1.51751184e+01 1.91275711e+01\n",
      " 1.04694853e+01 6.74751848e-02 8.05502415e+00 1.88317127e+01\n",
      " 1.74250355e+01 6.67761564e-02 7.87592125e+00 1.78794575e+01\n",
      " 1.49397182e+01 2.90289037e-02 1.62486229e+01 9.04964507e-02\n",
      " 1.89318237e+01 1.89318237e+01 1.52142696e+01 1.89318237e+01\n",
      " 2.07440033e+01 1.86359692e+01 1.89318237e+01 1.49397182e+01\n",
      " 1.22999878e+01 2.02076836e+01 1.86359692e+01 1.77208710e+01\n",
      " 1.18690478e-02 2.89304722e-02 1.42445434e-02 9.04964507e-02\n",
      " 7.34143686e+00 7.87592125e+00 2.84547567e+00 1.71805439e+01\n",
      " 1.89318237e+01 1.91275711e+01 7.71816015e+00 1.89318237e+01\n",
      " 2.03384895e+01 1.89318237e+01 1.91275711e+01 1.52658867e-02\n",
      " 1.88427353e+01 8.05502415e+00 7.87343502e+00 1.31199064e+01\n",
      " 4.13406610e+00 2.12193704e+00 6.74751848e-02 6.74751848e-02\n",
      " 1.18853104e+00 1.78923059e+00 8.05502415e+00 9.04964507e-02\n",
      " 9.77560997e+00 1.28583822e+01 1.05643358e+01 2.63486337e-02\n",
      " 1.52787142e+01 1.25380011e+01 2.31751728e+01 2.38928928e+01\n",
      " 7.34143686e+00 8.96824074e+00 1.74250355e+01 4.55477089e-02\n",
      " 1.00655289e+01 1.67029667e+01 1.89318237e+01 1.78923059e+00\n",
      " 9.04964507e-02]\n",
      "2 (101, 16)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(cost_i, feed_dict = feed))\n",
    "    print(len(sess.run([cost_i, cost], feed_dict = feed)), x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
